import os
import torch
from .model_torch import ModelTorch


class NeuralNetwork(ModelTorch):
    """Neural Network model.

    A feed-forward neural network model.

    Parameters
    ----------
    descriptor: object
        A descriptor that transforms atomic environment information to the
        fingerprints, which are used as the input for the neural network.

    seed: int (optional)
        Global seed for random numbers.
    """

    def __init__(self, descriptor, seed=35):
        super(NeuralNetwork, self).__init__(descriptor, seed)

        self.layers = None

    def add_layers(self, *layers):
        """Add layers to the sequential model.

        Parameters
        ----------
        layers: torch.nn layers
            ``torch.nn`` layers that are used to build a sequential model.
            Available ones including: torch.nn.Linear, torch.nn.Dropout, and
            torch.nn.Sigmoid among others. See
            https://pytorch.org/docs/stable/nn.html
            for a full list of torch.nn layers.
        """
        if self.layers is not None:
            raise NeuralNetworkError(
                '"add_layers" called multiple times. It should be called only once.'
            )
        else:
            self.layers = []

        for la in layers:
            self.layers.append(la)
            # set it as attr so that parameters are automatically registered
            setattr(self, 'layer_{}'.format(len(self.layers)), la)

        # check shape of first layer and last layer
        first = self.layers[0]
        if first.in_features != len(self.descriptor):
            raise NeuralNetworkError(
                '"in_features" of first layer should be equal to descriptor size.'
            )
        last = self.layers[-1]
        if last.out_features != 1:
            raise NeuralNetworkError('"out_features" of last layer should be 1.')

        # cast types
        self.type(self.dtype)

    def forward(self, x):
        for j, layer in enumerate(self.layers):
            x = layer(x)
        return x

    # TODO check whether transposed applied to linear layer weight matrix
    # see https://pytorch.org/docs/stable/nn.html#linear
    def write_kim_model(self, path=None):
        # supported
        param_layer = ['Linear']
        activ_layer = ['Sigmoid', 'Tanh', 'ReLU', 'ELU']
        dropout_layer = ['Dropout']
        layer_groups = self._group_layers(param_layer, activ_layer, dropout_layer)

        weights, biases = self._get_weights_and_biases(layer_groups, param_layer)
        activations = self._get_activations(layer_groups, activ_layer)
        drop_ratios = self._get_drop_ratios(layer_groups, dropout_layer)

        descriptor = self.descriptor
        dtype = self.dtype

        if path is None:
            path = os.path.join(os.getcwd(), 'NeuralNetwork__MO_000000111111_000')
        if path and not os.path.exists(path):
            os.makedirs(path)

        # TODO write CMakeList and probably pull the Eigen library
        with open(os.path.join(path, 'CMakeLists.txt'), 'w') as fout:
            fout.write('to be filled')

        # write parameter file
        fname = 'kliff_trained.params'
        with open(os.path.join(path, fname), 'w') as fout:
            fout.write('#' + '=' * 80 + '\n')
            fout.write(
                '# KIM ANN potential parameters, generated by `kliff` '
                'fitting program.\n'
            )
            fout.write('#' + '=' * 80 + '\n\n')

            # cutoff
            cutname, rcut = descriptor.get_cutoff()
            maxrcut = max(rcut.values())

            fout.write('# cutoff    rcut\n')
            if dtype == torch.float64:
                fout.write('{}  {:.15g}\n\n'.format(cutname, maxrcut))
            else:
                fout.write('{}  {:.7g}\n\n'.format(cutname, maxrcut))

            # symmetry functions
            # header
            fout.write('#' + '=' * 80 + '\n')
            fout.write('# symmetry functions\n')
            fout.write('#' + '=' * 80 + '\n\n')

            desc = descriptor.get_hyperparams()
            num_desc = len(desc)
            fout.write('{}    #number of symmetry funtion types\n\n'.format(num_desc))

            # descriptor values
            fout.write('# sym_function    rows    cols\n')
            for name, values in desc.items():
                if name == 'g1':
                    fout.write('g1\n\n')
                else:
                    rows = len(values)
                    cols = len(values[0])
                    fout.write('{}    {}    {}\n'.format(name, rows, cols))
                    if name == 'g2':
                        for val in values:
                            if dtype == torch.float64:
                                fout.write('{:.15g} {:.15g}'.format(val[0], val[1]))
                            else:
                                fout.write('{:.7g} {:.7g}'.format(val[0], val[1]))
                            fout.write('    # eta  Rs\n')
                        fout.write('\n')
                    elif name == 'g3':
                        for val in values:
                            if dtype == torch.float64:
                                fout.write('{:.15g}'.format(val[0]))
                            else:
                                fout.write('{:.7g}'.format(val[0]))
                            fout.write('    # kappa\n')
                        fout.write('\n')
                    elif name == 'g4':
                        for val in values:
                            zeta = val[0]
                            lam = val[1]
                            eta = val[2]
                            if dtype == torch.float64:
                                fout.write(
                                    '{:.15g} {:.15g} {:.15g}'.format(zeta, lam, eta)
                                )
                            else:
                                fout.write('{:.7g} {:.7g} {:.7g}'.format(zeta, lam, eta))
                            fout.write('    # zeta  lambda  eta\n')
                        fout.write('\n')
                    elif name == 'g5':
                        for val in values:
                            zeta = val[0]
                            lam = val[1]
                            eta = val[2]
                            if dtype == torch.float64:
                                fout.write(
                                    '{:.15g} {:.15g} {:.15g}'.format(zeta, lam, eta)
                                )
                            else:
                                fout.write('{:.7g} {:.7g} {:.7g}'.format(zeta, lam, eta))
                            fout.write('    # zeta  lambda  eta\n')
                        fout.write('\n')

            # data centering and normalization
            # header
            fout.write('#' + '=' * 80 + '\n')
            fout.write('# Preprocessing data to center and normalize\n')
            fout.write('#' + '=' * 80 + '\n')

            # mean and stdev
            mean = descriptor.get_mean()
            stdev = descriptor.get_stdev()
            if mean is None and stdev is None:
                fout.write('center_and_normalize  False\n')
            else:
                fout.write('center_and_normalize  True\n\n')

                fout.write('# mean\n')
                for i in mean:
                    if dtype == torch.float64:
                        fout.write('{:23.15e}\n'.format(i))
                    else:
                        fout.write('{:15.7e}\n'.format(i))
                fout.write('\n# standard deviation\n')
                for i in stdev:
                    if dtype == torch.float64:
                        fout.write('{:23.15e}\n'.format(i))
                    else:
                        fout.write('{:15.7e}\n'.format(i))
                fout.write('\n')

            # ann structure and parameters
            # header
            fout.write('#' + '=' * 80 + '\n')
            fout.write('# ANN structure and parameters\n')
            fout.write('#\n')
            fout.write(
                '# Note that the ANN assumes each row of the input "X" '
                'is an observation, i.e.\n'
            )
            fout.write('# the layer is implemented as\n')
            fout.write('# Y = activation(XW + b).\n')
            fout.write(
                '# You need to transpose your weight matrix if each '
                'column of "X" is an observation.\n'
            )
            fout.write('#' + '=' * 80 + '\n\n')

            # number of layers
            num_layers = len(weights)
            fout.write(
                '{}    # number of layers (excluding input layer, '
                'including output layer)\n'.format(num_layers)
            )

            # size of layers
            for b in biases:
                fout.write('{}  '.format(len(b)))
            fout.write('  # size of each layer (last must be 1)\n')

            # activation function
            # TODO enable writing different activations for each layer
            activation = activations[0]
            fout.write('{}    # activation function\n'.format(activation))

            # keep probability
            for i in drop_ratios:
                fout.write('{:.15g}  '.format(1.0 - i))
            fout.write('  # keep probability of input for each layer\n\n')

            # weights and biases
            for i, (w, b) in enumerate(zip(weights, biases)):

                # weight
                rows, cols = w.shape
                if i != num_layers - 1:
                    fout.write(
                        '# weight of hidden layer {} (shape({}, {}))\n'.format(
                            i + 1, rows, cols
                        )
                    )
                else:
                    fout.write(
                        '# weight of output layer (shape({}, {}))\n'.format(rows, cols)
                    )
                for line in w:
                    for item in line:
                        if dtype == torch.float64:
                            fout.write('{:23.15e}'.format(item))
                        else:
                            fout.write('{:15.7e}'.format(item))
                    fout.write('\n')

                # bias
                if i != num_layers - 1:
                    fout.write(
                        '# bias of hidden layer {} (shape({}, {}))\n'.format(
                            i + 1, rows, cols
                        )
                    )
                else:
                    fout.write(
                        '# bias of output layer (shape({}, {}))\n'.format(rows, cols)
                    )
                for item in b:
                    if dtype == torch.float64:
                        fout.write('{:23.15e}'.format(item))
                    else:
                        fout.write('{:15.7e}'.format(item))
                fout.write('\n\n')

    def _group_layers(self, param_layer, activ_layer, dropout_layer):
        """Divide all the layers into groups.

        The first group is either an empty list or a `Dropout` layer for the
        input layer. The last group typically contains only a `Linear` layer.
        For other groups, each group contains two, or three layers. `Linear`
        layer and an activation layer are mandatory, and a third `Dropout` layer
        is optional.

        Return
        ------
        groups: list of list of layers
        """

        groups = []
        new_group = []

        supported = param_layer + activ_layer + dropout_layer
        for i, layer in enumerate(self.layers):
            name = layer.__class__.__name__
            if name not in supported:
                raise NeuralNetworkError(
                    'Layer "{}" not supported by KIM model. Cannot proceed '
                    'to write.'.format(name)
                )
            if name in activ_layer:
                if i == 0:
                    raise NeuralNetworkError(
                        'First layer cannot be a "{}" layer'.format(name)
                    )
                if self.layers[i - 1].__class__.__name__ not in param_layer:
                    raise NeuralNetworkError(
                        'Cannot convert to KIM model. a "{}" layer must follow '
                        'a "Linear" layer.'.format(name)
                    )
            if name[:7] in dropout_layer:
                if self.layers[i - 1].__class__.__name__ not in activ_layer:
                    raise NeuralNetworkError(
                        'Cannot convert to KIM model. a "{}" layer must follow '
                        'an activation layer.'.format(name)
                    )
            if name in param_layer:
                groups.append(new_group)
                new_group = []
            new_group.append(layer)
        groups.append(new_group)

        return groups

    @staticmethod
    def _get_weights_and_biases(groups, supported):
        """Get weights and biases of all layers that have weights and biases."""
        weights = []
        biases = []
        for i, g in enumerate(groups):
            if i != 0:
                layer = g[0]
                name = layer.__class__.__name__
                if name in supported:
                    weight = layer.weight
                    bias = layer.bias
                    weights.append(weight)
                    biases.append(bias)
        return weights, biases

    @staticmethod
    def _get_activations(groups, supported):
        """Get the activation of all layers."""
        activations = []
        for i, g in enumerate(groups):
            if i != 0 and i != (len(groups) - 1):
                layer = g[1]
                name = layer.__class__.__name__
                if name in supported:
                    activations.append(name.lower())
        return activations

    @staticmethod
    def _get_drop_ratios(groups, supported):
        """Get the dropout ratio of all layers."""
        drop_ratios = []
        for i, g in enumerate(groups):
            if i == 0:
                if len(g) != 0:
                    layer = g[0]
                    name = layer.__class__.__name__
                    if name in supported:
                        drop_ratios.append(layer.p)
                else:
                    drop_ratios.append(0.0)
            elif i == len(groups) - 1:
                pass
            else:
                if len(g) == 3:
                    layer = g[2]
                    name = layer.__class__.__name__
                    if name in supported:
                        drop_ratios.append(layer.p)
                else:
                    drop_ratios.append(0.0)

        return drop_ratios


class NeuralNetworkError(Exception):
    def __init__(self, msg):
        super(NeuralNetworkError, self).__init__(msg)
        self.msg = msg

    def __expr__(self):
        return self.msg
