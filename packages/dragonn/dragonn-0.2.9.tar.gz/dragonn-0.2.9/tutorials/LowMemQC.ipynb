{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LowMemQC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kundajelab/dragonn/blob/master/paper_supplement/LowMemQC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PgZwwRreD9P9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cd3cecf0-42f6-4d03-b5ed-a5430d3a903e"
      },
      "cell_type": "code",
      "source": [
        "#uncomment the lines below if you are running this tutorial from Google Colab \n",
        "!pip install dragonn>=0.2.6\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mseqdataloader 0.121 has requirement pandas>=0.23.4, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aK_u2unQD9QD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Making sure our results are reproducible\n",
        "from numpy.random import seed\n",
        "seed(1234)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdnEx_EiD9QE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a223c4f-5098-498e-948c-184a798c59b6"
      },
      "cell_type": "code",
      "source": [
        "#load dragonn tutorial utilities \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from dragonn.tutorial_utils import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CIyS9aXkD9QI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Input data <a name='1'>\n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "Tutorials 1 - 3 have used simulated data generated with the simdna package. In this tutorial, we will examine how well CNN's are able to predict transcription factor binding for four TF's in vivo. \n",
        "\n",
        "We will learn to predict transcription factor binding for four transcription factors in the GM12878 cell line (one of the Tier 1 cell lines for the ENCODE project). First, we download the narrowPeak bed files for each of these transcription factors. You can skip the following code block if you already have the data downloaded. "
      ]
    },
    {
      "metadata": {
        "id": "joKrWPskD9QJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "d9c814ad-2728-43d7-88f1-e760f82f56de"
      },
      "cell_type": "code",
      "source": [
        "## SPI1, optimal IDR thresholded peaks, Myers lab, hg19\n",
        "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
        "#!wget -O SPI1.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.narrowPeak.gz\n",
        "    \n",
        "## Download \"ambiguous\" peak sets -- these peaks are in the optimal overlap set across replicates, but are not\n",
        "## found to be reproducible at a high confidence (p<0.05) by IDR \n",
        "#! wget -O SPI1.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.ambiguous.gz\n",
        "    \n",
        "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.chrom.sizes\n",
        "    \n",
        "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
        "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
        "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.fai \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-28 08:29:34--  http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.chrom.sizes\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 365\n",
            "Saving to: ‘hg19.chrom.sizes’\n",
            "\n",
            "\rhg19.chrom.sizes      0%[                    ]       0  --.-KB/s               \rhg19.chrom.sizes    100%[===================>]     365  --.-KB/s    in 0s      \n",
            "\n",
            "2019-02-28 08:29:34 (58.6 MB/s) - ‘hg19.chrom.sizes’ saved [365/365]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xPNOawx_D9QL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating positive and negative bins for genome-wide training <a name='2'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "metadata": {
        "id": "T5YXbDhrD9QM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. We will treat each sample as a task for the model and compare the performance of the model on SPI1 task in the single-tasked and multi-tasked setting."
      ]
    },
    {
      "metadata": {
        "id": "ty7OVrz2D9QO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from seqdataloader import * "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P2MY4tuCD9QR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ecd61735-2357-49a1-e480-7338fa116d9a"
      },
      "cell_type": "code",
      "source": [
        "## seqdataloader accepts an input file, which we call tasks.tsv, with task names in column 1, the corresponding\n",
        "## peak files in column 2, skip column 3 (which will be used for regression in Tutorial 5), and ambiguous peaks in \n",
        "## column4 \n",
        "with open(\"tasks.tsv\",'w') as f: \n",
        "  f.write(\"\\t\".join([\"SPI1\",\"SPI1.narrowPeak.gz\",\"\",\"SPI1.ambiguous.gz\"])+'\\n')\n",
        "\n",
        "! cat tasks.tsv"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SPI1\tSPI1.narrowPeak.gz\t\tSPI1.ambiguous.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "794ATwuBD9QV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With the parameter configuration below, seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
        "\n",
        "* Each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. \n",
        "\n",
        "* The bin is labeled ambiguous (label = -1) and excluded from training if there is some overlap with the narrowPeak, but the peak summit does not lie in that overlap. \n",
        "\n",
        "* The bin is labeled negative if there is no overlap with the narrowPeak. "
      ]
    },
    {
      "metadata": {
        "id": "YNZ6kU5INojl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89313292-1d6b-4098-d845-9732f774c404"
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q5JWEtjXD9QW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "4c143a7a-e45f-4480-e0e3-56ce82cc2de9"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "positives_train_set_params={\n",
        "    'store_positives_only':True,\n",
        "    'task_list':\"tasks.tsv\",\n",
        "    'outf':\"positives.TF.train.hdf5\",\n",
        "    'output_type':'hdf5',\n",
        "    'chrom_sizes':'hg19.chrom.sizes',\n",
        "    'chroms_to_exclude':['chr1','chr2','chr19'],\n",
        "    'bin_stride':50,\n",
        "    'left_flank':400,\n",
        "    'right_flank':400,\n",
        "    'bin_size':200,\n",
        "    'threads':20,\n",
        "    'subthreads':2,\n",
        "    'allow_ambiguous':True,\n",
        "    'output_hdf5_low_mem':True,\n",
        "    'labeling_approach':'peak_summit_in_bin_classification'\n",
        "    }\n",
        "genomewide_labels(positives_train_set_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating dictionary of bed files and bigwig files for each task:\n",
            "SPI1\n",
            "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
            "creating chromosome thread pool\n",
            "launching thread pool\n",
            "pre-allocated df for chrom:chr21with dimensions:(962578, 4)\n",
            "pre-allocated df for chrom:chr22with dimensions:(1026072, 4)pre-allocated df for chrom:chr18with dimensions:(1561525, 4)\n",
            "\n",
            "pre-allocated df for chrom:chr17with dimensions:(1623885, 4)\n",
            "pre-allocated df for chrom:chr15with dimensions:(2050608, 4)pre-allocated df for chrom:chr16with dimensions:(1807076, 4)\n",
            "\n",
            "pre-allocated df for chrom:chr20with dimensions:(1260491, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aqbArOrhD9QY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "positives_valid_set_params={\n",
        "    'store_positives_only':True,\n",
        "    'task_list':\"tasks.tsv\",\n",
        "    'outf':\"positives.TF.valid.hdf5\",\n",
        "    'output_type':'hdf5',\n",
        "    'chrom_sizes':'hg19.chrom.sizes',\n",
        "    'chroms_to_keep':'chr1',\n",
        "    'bin_stride':50,\n",
        "    'left_flank':400,\n",
        "    'right_flank':400,\n",
        "    'bin_size':200,\n",
        "    'threads':20,\n",
        "    'subthreads':2,\n",
        "    'allow_ambiguous':True,\n",
        "    'output_hdf5_low_mem':True,\n",
        "    'labeling_approach':'peak_summit_in_bin_classification'\n",
        "    }\n",
        "genomewide_labels(positives_valid_set_params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xAOzk3rdD9Qb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "positives_test_set_params={\n",
        "    'store_positives_only':True,\n",
        "    'task_list':\"tasks.tsv\",\n",
        "    'outf':\"positives.TF.test.hdf5\",\n",
        "    'output_type':'hdf5',\n",
        "    'chrom_sizes':'hg19.chrom.sizes',\n",
        "    'chroms_to_keep':['chr2','chr19'],\n",
        "    'bin_stride':50,\n",
        "    'left_flank':400,\n",
        "    'right_flank':400,\n",
        "    'bin_size':200,\n",
        "    'threads':20,\n",
        "    'subthreads':2,\n",
        "    'allow_ambiguous':True,\n",
        "    'output_hdf5_low_mem':True,\n",
        "    'labeling_approach':'peak_summit_in_bin_classification'\n",
        "    }\n",
        "genomewide_labels(positives_test_set_params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyDOKpsDD9Qf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# When provided with the --store-positives_only flag, the code generates all bins for each task that are labeled positive.\n",
        "pd.read_hdf(\"SPI1.positives.TF.train.hdf5\",start=0,stop=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eAjuqJM1D9Qj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from dragonn.generators import * "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gCHAgATXD9Ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#To prepare for model training, we import the necessary functions and submodules from keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
        "import keras.losses;\n",
        "from keras.constraints import maxnorm;\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.callbacks import EarlyStopping, History, TensorBoard \n",
        "from keras import backend as K \n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "#we use a custom binary cross-entropy loss that can handle ambiguous labels (denoted with -1 ) and exclude them \n",
        "# from the loss calculation \n",
        "from dragonn.custom_losses import get_ambig_binary_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MiPHtJcsD9Qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
        "from keras.constraints import max_norm\n",
        "\n",
        "def initialize_model(ntasks=1):\n",
        "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
        "    model=Sequential() \n",
        "    \n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Dense(ntasks))\n",
        "    model.add(Activation(\"sigmoid\"))\n",
        "    \n",
        "    #use the custom ambig_binary_crossentropy loss, indicating that a value of -1 indicates an ambiguous label \n",
        "    loss=get_ambig_binary_crossentropy(-1)\n",
        "    \n",
        "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
        "    model.compile(optimizer='adam', loss=loss,\n",
        "                  metrics=[tpr,\n",
        "                           tnr,\n",
        "                           fpr,\n",
        "                           fnr,\n",
        "                           precision,\n",
        "                           f1])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSbcq-9jD9Qq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create the generators\n",
        "from dragonn.generators import * \n",
        "case1_spi1_train_gen=DataGenerator(\"SPI1.positives.TF.train.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
        "case1_spi1_valid_gen=DataGenerator(\"SPI1.positives.TF.valid.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
        "case1_ctcf_train_gen=DataGenerator(\"CTCF.positives.TF.train.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
        "case1_ctcf_valid_gen=DataGenerator(\"CTCF.positives.TF.valid.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4OsGDSAD9Qt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now follow the standard protocol we used in tutorials 1 - 3 to train a keras model, with the exception that we use the fit_generator function in keras, rather than the fit function."
      ]
    },
    {
      "metadata": {
        "id": "HJVdXu_ED9Qu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2x6by142D9Qz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#If you are running this notebook in google colab, uncomment the lines below to observe the model's training \n",
        "!mkdir logs\n",
        "%tensorboard --logdir logs \n",
        "tensorboard_visualizer=TensorBoard(log_dir=\"logs\", histogram_freq=0, batch_size=500, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
        "callbacks.append(tensorboard_visualizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bt6twhsVD9Q4",
        "colab_type": "code",
        "colab": {},
        "outputId": "e5465ed2-5cd8-40a7-882c-5a2dd6f9d2e3"
      },
      "cell_type": "code",
      "source": [
        "#Train the SPI1 model \n",
        "case1_spi1_model=initialize_model()\n",
        "\n",
        "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
        "history_case1_spi1=case1_spi1_model.fit_generator(case1_spi1_train_gen,\n",
        "                                                  validation_data=case1_spi1_valid_gen,\n",
        "                                                  epochs=150,\n",
        "                                                  verbose=1,\n",
        "                                                  use_multiprocessing=True,\n",
        "                                                  workers=40,\n",
        "                                                  max_queue_size=100,\n",
        "                                                  callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "2156/2156 [==============================] - 94s 43ms/step - loss: 0.0321 - sensitivity: 0.9871 - specificity: 0.9902 - fpr: 0.0098 - fnr: 0.0129 - precision: 0.9898 - f1: 0.9883 - val_loss: 0.2462 - val_sensitivity: 0.8540 - val_specificity: 1.0000 - val_fpr: 3.0800e-05 - val_fnr: 0.1460 - val_precision: 1.0000 - val_f1: 0.9190\n",
            "Epoch 2/150\n",
            "2156/2156 [==============================] - 89s 41ms/step - loss: 0.0075 - sensitivity: 0.9974 - specificity: 0.9977 - fpr: 0.0023 - fnr: 0.0026 - precision: 0.9977 - f1: 0.9976 - val_loss: 0.0135 - val_sensitivity: 0.9976 - val_specificity: 0.9933 - val_fpr: 0.0067 - val_fnr: 0.0024 - val_precision: 0.9934 - val_f1: 0.9955\n",
            "Epoch 3/150\n",
            "2156/2156 [==============================] - 93s 43ms/step - loss: 0.0060 - sensitivity: 0.9979 - specificity: 0.9979 - fpr: 0.0021 - fnr: 0.0021 - precision: 0.9979 - f1: 0.9979 - val_loss: 0.0185 - val_sensitivity: 0.9889 - val_specificity: 0.9997 - val_fpr: 2.7720e-04 - val_fnr: 0.0111 - val_precision: 0.9997 - val_f1: 0.9942\n",
            "Epoch 4/150\n",
            "2156/2156 [==============================] - 95s 44ms/step - loss: 0.0045 - sensitivity: 0.9985 - specificity: 0.9984 - fpr: 0.0016 - fnr: 0.0015 - precision: 0.9984 - f1: 0.9984 - val_loss: 0.0126 - val_sensitivity: 0.9922 - val_specificity: 0.9992 - val_fpr: 7.6999e-04 - val_fnr: 0.0078 - val_precision: 0.9992 - val_f1: 0.9957\n",
            "Epoch 5/150\n",
            "2156/2156 [==============================] - 93s 43ms/step - loss: 0.0037 - sensitivity: 0.9988 - specificity: 0.9987 - fpr: 0.0013 - fnr: 0.0012 - precision: 0.9987 - f1: 0.9987 - val_loss: 0.0074 - val_sensitivity: 0.9962 - val_specificity: 0.9992 - val_fpr: 7.6999e-04 - val_fnr: 0.0038 - val_precision: 0.9992 - val_f1: 0.9977\n",
            "Epoch 6/150\n",
            "2156/2156 [==============================] - 97s 45ms/step - loss: 0.0034 - sensitivity: 0.9989 - specificity: 0.9988 - fpr: 0.0012 - fnr: 0.0011 - precision: 0.9988 - f1: 0.9989 - val_loss: 0.0087 - val_sensitivity: 0.9996 - val_specificity: 0.9938 - val_fpr: 0.0062 - val_fnr: 3.6959e-04 - val_precision: 0.9939 - val_f1: 0.9967\n",
            "Epoch 7/150\n",
            "2156/2156 [==============================] - 97s 45ms/step - loss: 0.0028 - sensitivity: 0.9992 - specificity: 0.9990 - fpr: 0.0010 - fnr: 8.2256e-04 - precision: 0.9990 - f1: 0.9991 - val_loss: 0.0077 - val_sensitivity: 0.9959 - val_specificity: 0.9996 - val_fpr: 3.6959e-04 - val_fnr: 0.0041 - val_precision: 0.9996 - val_f1: 0.9977\n",
            "Epoch 8/150\n",
            "2156/2156 [==============================] - 97s 45ms/step - loss: 0.0027 - sensitivity: 0.9992 - specificity: 0.9990 - fpr: 9.6750e-04 - fnr: 7.6096e-04 - precision: 0.9990 - f1: 0.9991 - val_loss: 0.0148 - val_sensitivity: 0.9911 - val_specificity: 0.9999 - val_fpr: 1.2320e-04 - val_fnr: 0.0089 - val_precision: 0.9999 - val_f1: 0.9955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EvgcnKtlD9Q9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Plot the learning curves for SPI1  \n",
        "from dragonn.tutorial_utils import plot_learning_curve\n",
        "plot_learning_curve(history_case1_spi1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}