{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kundajelab/dragonn/blob/master/paper_supplement/PrimerTutorial%205%20-%20Functional%20variant%20characterization%20for%20non-coding%20SNPs%20within%20the%20SPI1%20motif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIu-8nkCV4zm"
   },
   "source": [
    "# How to train your DragoNN tutorial 5: \n",
    "## Functional variant characterization for non-coding SNPs within the SPI1 motif \n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript. \n",
    "\n",
    "This tutorial will take 2 - 3 hours if executed on a GPU.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>Input data: SPI1 ChiP-seq and experimental bQTL data</a></li>\n",
    "    <li><a href=#2>Genomewide classification and regression labels for SPI1 TF ChiPseq</a></li>\n",
    "    <li><a href=#3>Optional: Download pre-generated models and test-set predictions</a></li>\n",
    "    <li><a href=#4>Genome-wide classification for SPI1</a></li>\n",
    "    <li><a href=#5>Genome-wide regression for SPI1</a></li> \n",
    "    <li><a href=#6>Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT</a></li>\n",
    "    <li><a href=#7>Recovering bQTL effect sizes: Classification vs Regression</a></li>\n",
    "    <li><a href=#8>Model-predicted SNP effect sizes vs bQTL effect sizes</a></li>\n",
    "    <li><a href=#a>Kat's architecture: Classification Model</a></li>\n",
    "    <li><a href=#b>Kat's architecture: Regression Model</a></li> \n",
    "    <li><a href=#9>Conclusions</a></li>    \n",
    "    <li><a href=#10>Save tutorial outputs</a></li>\n",
    "</ol>\n",
    "Github issues on the [dragonn repository](https://github.com/kundajelab/dragonn) with feedback, questions, and discussion are always welcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72XlYRZBluGr"
   },
   "outputs": [],
   "source": [
    "# If you don't have bedtools installed in your environment (i.e. Google Colab), uncomment and run the command below \n",
    "#!apt-get install bedtools\n",
    "#!pip install pybedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmftiCCDV4zo"
   },
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install dragonn>=0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyLzeiF5V4zq"
   },
   "outputs": [],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8M6gdfuJV4zu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/users/annashch/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from dragonn.tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djDLAi21V4zy"
   },
   "source": [
    "## Input data <a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "This tutorial uses the same in vivo SPI1 transcription factor CHiP-seq dataset that was used in [Tutorial 4](https://colab.research.google.com/github/kundajelab/dragonn/blob/keras_2.2_tensorflow_1.6_purekeras/paper_supplement/PrimerTutorial%204%20-%20Interpreting%20predictive%20sequence%20features%20in%20in-vivo%20TF%20binding%20events.ipynb). Our goal is to compare predicted variant effect sizes from classification and regression models against experimental bQTL data. The bQTL data in this way serves as a \"gold-standard\" validation that in silico mutagenesis on the deep learning inputs leads to correct variant effect size prediction.  We  will use bQTL data  that has been intersected with SPI1 CISBP genome motif annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "O707uf21V4zy",
    "outputId": "f5ca2dbc-9594-4a62-aa67-97190945d622"
   },
   "outputs": [],
   "source": [
    "# SPI1, optimal IDR thresholded peaks, Myers lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
    "!wget -O SPI1.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "\n",
    "#Fold change bigWig track for the SPI1 dataset: \n",
    "!wget -O SPI1.pooled.fc.bigWig http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-macs2_pooled/execution/ENCFF000OBU.Rep1.merged.nodup.pooled_x_ENCFF000OCW.Control.Rep1.merged.nodup.fc.signal.bigwig\n",
    "    \n",
    "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.chrom.sizes\n",
    "    \n",
    "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz.fai \n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz.gzi \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "-YwnqCV-V4z2",
    "outputId": "85791b16-8647-45ff-cd8b-cf281d618350"
   },
   "outputs": [],
   "source": [
    "# Download bQTL experimental data for SPI1 loci \n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sp9mi-6_V4z4"
   },
   "source": [
    "## Generating genome-wide classification and regression labels <a name='2'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zmt5OJP_V4z5"
   },
   "source": [
    "We will use the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. We will treat each sample as a task for the model and compare the performance of the model on SPI1 task in the single-tasked and multi-tasked setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLGpH2rOV4z6"
   },
   "outputs": [],
   "source": [
    "from seqdataloader import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "u2wpRugxV4z7",
    "outputId": "c695d444-b6d0-408b-f294-6afcdc6b1033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPI1\tSPI1.narrowPeak.gz\tSPI1.pooled.fc.bigWig\r\n"
     ]
    }
   ],
   "source": [
    "## seqdataloader accepts an input file, which we call SPI1.tasks.tsv, with task names in column 1, corresponding\n",
    "## peak files in column 2, and the signal track in column 3. In this tutorial, the task file will have a single task entry for the SPI1 TF CHiP-seq\n",
    "with open(\"SPI1.task.tsv\",'w') as f: \n",
    "    f.write(\"SPI1\\tSPI1.narrowPeak.gz\\tSPI1.pooled.fc.bigWig\\n\")\n",
    "f.close() \n",
    "!cat SPI1.task.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pqz2oVGV4z_"
   },
   "source": [
    "With the parameter configuration below, seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
    "\n",
    "* In the classification case, each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. The bin is labeled negative if there is no overlap with the narrowPeak. \n",
    "* In the regression case, the asinh(mean coverage) in the 200 bp bin is computed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e--f8QWuV4z_"
   },
   "source": [
    "**Note**: The label generation may take 10 - 15 minutes to complete. If you prefer not to wait, you can download the \n",
    "pre-generated classification and regression labels for the training, validation, and test sets by uncommenting the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1205
    },
    "colab_type": "code",
    "id": "LMG9IzPnV40A",
    "outputId": "aca1e05d-dc62-416e-b22f-b68a53aaf3f7"
   },
   "outputs": [],
   "source": [
    "## Classification labels \n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.classification.hdf5\n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.classification.hdf5\n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.classification.hdf5\n",
    "\n",
    "## Regression labels \n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.regression.hdf5\n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.regression.hdf5\n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.regression.hdf5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lleRdAaV40B"
   },
   "source": [
    "If you prefer to generate the labels from scratch, execute the two code cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erh3B4hIV40D"
   },
   "outputs": [],
   "source": [
    "#  Generate genome-wide classification labels \n",
    "\n",
    "#1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set. Also, the dataset does not\n",
    "# include chromosome Y, so we exclude it as well. \n",
    "\n",
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBqbysuAV40G"
   },
   "outputs": [],
   "source": [
    "# Generate regression labels genome-wide \n",
    "\n",
    "#1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set \n",
    "\n",
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1m9HsgXV40J"
   },
   "source": [
    "Let's examine the files that were generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "Q0SgKnZtV40J",
    "outputId": "5e5501e6-61e3-44e1-f6ca-94e70f3ff63f"
   },
   "outputs": [],
   "source": [
    "#The code generates bed file outputs with a label of 1 or 0 for each 1kb\n",
    "# genome bin for each task. Note that the bins are shifted with a stride of 50.\n",
    "pd.read_hdf(\"SPI1.train.classification.hdf5\",start=1000000,stop=1000010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "weH35tmhV40N",
    "outputId": "a8aea293-f560-400f-cb2f-2d0085625934"
   },
   "outputs": [],
   "source": [
    "pd.read_hdf(\"SPI1.train.regression.hdf5\",start=1000000,stop=1000010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKHBBFpRV40Q"
   },
   "source": [
    "## Optional: Download pre-generated models and test-set predictions <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "Next, we will train classification and regression models to predict TF CHiP-seq peaks for SPI1. If you want to skip straight to model interpretation and bQTL analysis, you can download the pre-trained models by uncommenting the \n",
    "block of code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqyIROINV40R"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "## Download classification model \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.classification.model.hdf5\n",
    "spi1_classification_model=load_model(\"SPI1.kat.classification.model.hdf5\")\n",
    "\n",
    "## Download regression model \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.regression.model.hdf5\n",
    "spi1_regression_model=load_model(\"SPI1.kat.regression.model.hdf5\")\n",
    "\n",
    "\n",
    "## Get test set classification model and regression model predictions \n",
    "#import h5py\n",
    "#test_set_predictions=h5py.File(\"SPI1.test.predictions.hdf5\")\n",
    "#spi1_test_classification_predictions=test_set_predictions['classification'].value \n",
    "#spi1_test_regression_predictions=test_set_predictions['regression'].value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZcwz5AmV40U"
   },
   "source": [
    "## Genome-wide classification model <a name='4'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZeBBukYGV40V"
   },
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXtJhYf2V40Z"
   },
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "def initialize_classification_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=(1,1000,4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ntasks))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',\n",
    "                  metrics=[tpr,\n",
    "                           tnr,\n",
    "                           fpr,\n",
    "                           fnr,\n",
    "                           precision,\n",
    "                           f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XY-g6ik9V40c"
   },
   "source": [
    "We create generators for the training and validation data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5PrSMLLiV40d",
    "outputId": "d7ae2438-6e8e-4fc0-c1b3-6bde39667604"
   },
   "outputs": [],
   "source": [
    "#create the generators, upsample positives to ensure they constitute 30% of each batch \n",
    "from dragonn.generators import * \n",
    "spi1_train_classification_gen=DataGenerator(\"SPI1.train.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3, batch_size=256)\n",
    "spi1_valid_classification_gen=DataGenerator(\"SPI1.valid.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLRfBhebV40e"
   },
   "outputs": [],
   "source": [
    "#Train the SPI1 classification model \n",
    "spi1_classification_model=initialize_classification_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_classification=spi1_classification_model.fit_generator(spi1_train_classification_gen,\n",
    "                                                  validation_data=spi1_valid_classification_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DN2bFz_aV40h",
    "outputId": "323bc266-fc71-4e30-e277-b2db3c3e9a0a"
   },
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "plot_learning_curve(history_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YoP336y_V40k"
   },
   "source": [
    "We now measure how well the model performed by calculating performance metrics on the test splits across the whole genome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZF6VNZH3V40k",
    "outputId": "101ca0a8-db49-44cc-c133-e799b070e1e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6047/6047 [==============================] - 419s 69ms/step\n"
     ]
    }
   ],
   "source": [
    "from dragonn.generators import * \n",
    "spi1_test_classification_gen=DataGenerator(\"SPI1.test.classification.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_classification_predictions=spi1_classification_model.predict_generator(spi1_test_classification_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_classification_truth=spi1_test_classification_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QGwsaLsV40n",
    "outputId": "829e46ff-12f4-42c9-c18c-2d9c66a22e97"
   },
   "outputs": [],
   "source": [
    "spi1_test_classification_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LAq8j_MV40r",
    "outputId": "d1b31200-f1a6-493b-ad99-6c0c62b65645"
   },
   "outputs": [],
   "source": [
    "spi1_test_classification_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X83Mt6VXV40t",
    "outputId": "b7b4fe79-812d-463a-f2f1-611122a6fa0e"
   },
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "from dragonn.metrics import ClassificationResult\n",
    "print(ClassificationResult(spi1_test_classification_truth.values.astype(bool),spi1_test_classification_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBvkHg7zV40y"
   },
   "outputs": [],
   "source": [
    "#save the models \n",
    "spi1_classification_model.save(\"SPI1.classification.model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0sibNElV403"
   },
   "source": [
    "## Genome-wide regression model <a name='5'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nM0-1aa9V404"
   },
   "outputs": [],
   "source": [
    "def initialize_regression_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=(1,1000,4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "    model.add(Conv2D(filters=10,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=5,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ntasks))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zO-uKK8GV407",
    "outputId": "04c1a178-b8ee-4b14-c944-328c17ecec9c"
   },
   "outputs": [],
   "source": [
    "#we want to determine a threshold for upsampling the non-zero bins in a given batch \n",
    "# extract 5 million datapoints from the training data and observe the distribution of non-zero signal values  \n",
    "sample=pd.read_hdf(\"SPI1.train.regression.hdf5\",start=0,stop=5000000)\n",
    "nonzero_sample=sample[sample.max(axis=1)>0]\n",
    "print(nonzero_sample.shape)\n",
    "nonzero_sample.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5Raqyz3V40_"
   },
   "source": [
    "This suggests that 0.1 is a reasonable threshold for upsampling signal bins in regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSjGIlYTV41B"
   },
   "outputs": [],
   "source": [
    "#create the generators, no upsampling of positives is used for regression. \n",
    "from dragonn.generators import * \n",
    "spi1_train_regression_gen=DataGenerator(\"SPI1.train.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.01)\n",
    "spi1_valid_regression_gen=DataGenerator(\"SPI1.valid.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3pSs4TYV41E",
    "outputId": "bb50b654-2336-4b6d-d16c-a8a15672f379"
   },
   "outputs": [],
   "source": [
    "#Train the SPI1 regression model \n",
    "spi1_regression_model=initialize_regression_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_regression=spi1_regression_model.fit_generator(spi1_train_regression_gen,\n",
    "                                                  validation_data=spi1_valid_regression_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EFxneNJqV41H",
    "outputId": "472b64dc-f85d-49e6-8bd0-4d023374e04a"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(history_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_o3I4gN7V41K",
    "outputId": "da95db30-a6ad-4043-86c6-ed587a44277b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6047/6047 [==============================] - 339s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "from dragonn.generators import * \n",
    "spi1_test_regression_gen=DataGenerator(\"SPI1.test.regression.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_regression_predictions=spi1_regression_model.predict_generator(spi1_test_regression_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_regression_truth=spi1_test_regression_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMbEZo8XGd2m"
   },
   "outputs": [],
   "source": [
    "## find the indices of the non-zero coverage bins \n",
    "nonzero_bins=spi1_test_regression_truth.max(axis=1)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-byNK4qMV41N",
    "outputId": "aa18b7e6-6ee2-439d-ca0b-d68d2a7aa8a1"
   },
   "outputs": [],
   "source": [
    "#Calculate spearman and pearson correlation between truth labels and predictions \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "corr_pearson=pearsonr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "corr_spearman=spearmanr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YipLiXRFGd2t",
    "outputId": "f7e2d334-30b4-4661-d8bb-23832c8c5763"
   },
   "outputs": [],
   "source": [
    "# Calculate the spearman and pearson correlation, restricted to non-zero bins \n",
    "corr_pearson_nonzero_bins=pearsonr(spi1_test_regression_truth[nonzero_bins],spi1_test_regression_predictions[nonzero_bins])\n",
    "corr_spearman_nonzero_bins=spearmanr(spi1_test_regression_truth[nonzero_bins],spi1_test_regression_predictions[nonzero_bins])\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson_nonzero_bins))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman_nonzero_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVEfZ1d0V41O"
   },
   "outputs": [],
   "source": [
    "#There is some overfitting, let's save this model and see if we can do better \n",
    "spi1_regression_model.save(\"SPI1.regression.model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGW2FQrgGd2y",
    "outputId": "e3069d0a-65d3-4c8c-be0d-eef77ed83581"
   },
   "outputs": [],
   "source": [
    "spi1_test_regression_truth.values[0:10].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKa34qjbGd21"
   },
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame({\"Observed\":list(spi1_test_regression_truth.values.squeeze()),\n",
    "                     \"Predicted\":list(spi1_test_regression_predictions.squeeze())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1tUvhU5Gd27"
   },
   "outputs": [],
   "source": [
    "test_df_nonzero=pd.DataFrame({\"Observed\":list(spi1_test_regression_truth[nonzero_bins].values.squeeze()),\n",
    "                     \"Predicted\":list(spi1_test_regression_predictions[nonzero_bins].squeeze())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xqvik10lGd29",
    "outputId": "4d35b751-13ce-4555-f798-916765d4e981"
   },
   "outputs": [],
   "source": [
    "import plotnine \n",
    "from plotnine import * \n",
    "print((ggplot(test_df,aes(x=\"Observed\",y=\"Predicted\"))\n",
    " +geom_bin2d(bins=100)\n",
    " +theme_bw()\n",
    " +xlab(\"Observed asinh(mean coverage in FC bigWig\")\n",
    " +ylab(\"Model prediction\")\n",
    " +ggtitle(\"SPI1 regression model test set prediction\")))\n",
    "\n",
    "print((ggplot(test_df_nonzero,aes(x=\"Observed\",y=\"Predicted\"))\n",
    " +geom_bin2d(bins=100)\n",
    " +theme_bw()\n",
    " +xlab(\"Observed asinh(mean coverage in FC bigWig\")\n",
    " +ylab(\"Model prediction\")\n",
    " +ggtitle(\"SPI1 regression model test set prediction: bins with nonzero coverage\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOBvin88Gd3A",
    "outputId": "5fc16ecd-16d2-47f0-a39f-e91244e4d9cb"
   },
   "outputs": [],
   "source": [
    "# Plot observed vs predicted regression values \n",
    "plt.scatter(spi1_test_regression_truth, spi1_test_regression_predictions, alpha=0.01)\n",
    "plt.xlabel(\"Observed asinh(mean coverage in FC bigWig)\")\n",
    "plt.ylabel(\"Model prediction\")\n",
    "plt.title(\"SPI1 regression model test set prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCgR4DmGGd3C",
    "outputId": "82246537-1073-48b1-81f7-1dc8f2a4b178"
   },
   "outputs": [],
   "source": [
    "# Plot observed vs predicted regression values for the nonzero bins \n",
    "plt.scatter(spi1_test_regression_truth, spi1_test_regression_predictions, alpha=0.01)\n",
    "plt.xlabel(\"Observed asinh(mean coverage in FC bigWig) for bins \")\n",
    "plt.ylabel(\"Model prediction\")\n",
    "plt.title(\"SPI1 regression model test set prediction: bins with nonzero coverage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48jGNSBtV41R"
   },
   "source": [
    "## Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT <a name='6'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "### Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWumi0mQV41S"
   },
   "outputs": [],
   "source": [
    "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence true positive predictions)\n",
    "spi1_test_classification_truth_bool=spi1_test_classification_truth.values.astype(bool)\n",
    "true_pos_spi1=spi1_test_classification_truth[spi1_test_classification_truth_bool*spi1_test_classification_predictions >0.9]\n",
    "true_pos_spi1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecJa0a2HV41U",
    "outputId": "bb860e8e-f083-41e3-ed79-e66bb8aec2ac"
   },
   "outputs": [],
   "source": [
    "true_pos_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UH5WvDmXV41W",
    "outputId": "c549e164-1087-4f2b-93bd-7f0677075090"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import one_hot_from_bed\n",
    "deep_lift_input_spi1=one_hot_from_bed([i for i in true_pos_spi1.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41dInIg-V41Y"
   },
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import deeplift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hViDtFpGV41a"
   },
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1=deeplift(spi1_classification_model,deep_lift_input_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFkS3UwpV41c",
    "outputId": "953cdbc2-d7c0-4a17-d6d1-227acb47c325"
   },
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skCZQ8j0V41e"
   },
   "source": [
    "Let's plot a few of the DeepLIFT tracks and see if the model successfully learned SPI1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Srt-dPa_V41e"
   },
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import  plot_seq_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhcSoq48V41h",
    "outputId": "fd5de43b-d65f-4abb-d998-7115c5ad1c1e"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eyejvCeWV41j",
    "outputId": "9985d83c-d05f-4979-f472-bb352cce2f80"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyuioSW_V41o",
    "outputId": "75f83e18-bf46-475d-c43b-22ab8cdc56d9"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfkuGJTGV41r"
   },
   "source": [
    "Let's zoom in to the center of one sequence so that it is easier to distinguish the motif: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCr3vtPeV41s",
    "outputId": "a04a10b2-ff18-44cb-f918-005872cb3b59"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[550:650],deep_lift_input_spi1[2].squeeze()[550:650])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tiw5YPInV41v"
   },
   "source": [
    "If we query the sequence \"CACTTCCCCT\" in the [TomTom](http://meme-suite.org/tools/tomtom) software from the MEME suite, we find that the motif is a good match for SPIB: \n",
    "<img src=\"https://github.com/kundajelab/dragonn/blob/master/paper_supplement/tutorial_images/SPI1.Tut4.png?raw=1\" alt=\"SPI12TomTom\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEUxg-r1V41x"
   },
   "source": [
    "### Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bEJudwWXV41y",
    "outputId": "f2395f38-be1f-4b76-95a3-91cc85efa471"
   },
   "outputs": [],
   "source": [
    "#Sanity-check that the model is learning the SPI1 motif by running DeepLIFT on True Positives with high confidence (>0.9)\n",
    "#get the true positive predictions \n",
    "true_pos=spi1_test_regression_truth[(spi1_test_regression_truth.values*spi1_test_regression_predictions)>2]\n",
    "true_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5dpuvzLV413",
    "outputId": "c0db98aa-22bf-4e44-f15f-0a75df7c9239"
   },
   "outputs": [],
   "source": [
    "deep_lift_input=one_hot_from_bed([i for i in true_pos.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-3kc90ZV416",
    "outputId": "514a9b2b-0f73-45ac-fe0c-b695bb182dbb"
   },
   "outputs": [],
   "source": [
    "help(deeplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeuZFMZqV41-"
   },
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1=deeplift(spi1_regression_model,deep_lift_input_spi1,target_layer_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qigmzDOV41-",
    "outputId": "1ef6a324-0b04-4000-a658-87c75cb0c50d"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPHe9I8VV42A",
    "outputId": "b7ee3839-2d4e-466b-e2f7-ede00db8a16b"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bogEKZN2V42C",
    "outputId": "6465c01d-b842-4217-9766-aca7a14797b2"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ck63kAsAV42F",
    "outputId": "5353b1fb-b881-4515-f931-96f49f196045"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[550:650],deep_lift_input_spi1[2].squeeze()[550:650])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWqxtR6NV42I"
   },
   "source": [
    "The motif learned by the regression model matches the canonical SPI1 motif, though the deepLIFT tracks are noisier compared to those for the classification model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PGA_k3RV42J"
   },
   "source": [
    "## Recovering bQTL effect sizes: Classification vs Regression <a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GROAoPZDV42J"
   },
   "outputs": [],
   "source": [
    "from dragonn.generators import * \n",
    "bqtl_ref_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"POSTallele\")\n",
    "bqtl_alt_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"ALTallele\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJq0Ic_8V42L",
    "outputId": "f27e9d3e-87a2-479f-90ca-320bf3066fc0"
   },
   "outputs": [],
   "source": [
    "bqtl_ref_classification_predictions=spi1_classification_model.predict_generator(bqtl_ref_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlPJpmmLV42L",
    "outputId": "bf8b8ff1-0268-49af-ce7d-9d8875e2ae3f"
   },
   "outputs": [],
   "source": [
    "bqtl_alt_classification_predictions=spi1_classification_model.predict_generator(bqtl_alt_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "bqtl_ref_classification_truth=bqtl_ref_gen.data['pvalue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGXJaSvPV42N",
    "outputId": "32d58541-15ab-4c82-90d5-b3118ed18520"
   },
   "outputs": [],
   "source": [
    "print(bqtl_ref_classification_predictions.shape)\n",
    "print(bqtl_alt_classification_predictions.shape)\n",
    "print(bqtl_ref_classification_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NpOM03tV42P",
    "outputId": "9e58fdff-c60a-463a-fa43-2a6979d96064"
   },
   "outputs": [],
   "source": [
    "bqtl_ref_regression_predictions=spi1_regression_model.predict_generator(bqtl_ref_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "bqtl_alt_regression_predictions=spi1_regression_model.predict_generator(bqtl_alt_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hi_Pr6jcV42Q",
    "outputId": "62710f23-734d-4ea4-b9a9-e1c28948d597"
   },
   "outputs": [],
   "source": [
    "plt.scatter(bqtl_ref_classification_predictions, bqtl_alt_classification_predictions, alpha=0.01)\n",
    "plt.xlabel(\"Ref\")\n",
    "plt.ylabel(\"Alt\")\n",
    "plt.title(\"BQTL Classification Model Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tg-ZRo1tV42R",
    "outputId": "9cb78eaf-ed5f-4fe4-ca02-6f927b07bcca"
   },
   "outputs": [],
   "source": [
    "plt.scatter(bqtl_ref_regression_predictions, bqtl_alt_regression_predictions, alpha=0.01)\n",
    "plt.xlabel(\"Ref\")\n",
    "plt.ylabel(\"Alt\")\n",
    "plt.title(\"BQTL Regression Model Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxG07_SzV42T"
   },
   "source": [
    "## Model-predicted SNP effect sizes vs bQTL effect sizes <a name='8'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdFUgn60V42T"
   },
   "outputs": [],
   "source": [
    "logpval=np.log10(bqtl_ref_classification_truth.values)\n",
    "delta=bqtl_alt_classification_predictions-bqtl_ref_classification_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdQsnAM7Gd4B"
   },
   "source": [
    "## Kat's Model Architecture (Classification)<a name='a'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yIyOg-AGd4B"
   },
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "def initialize_kat_classification_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(ntasks))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=[tpr,\n",
    "                           tnr,\n",
    "                           fpr,\n",
    "                           fnr,\n",
    "                           precision,\n",
    "                           f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EG64R1eNGd4C"
   },
   "outputs": [],
   "source": [
    "#create the generators, upsample positives to ensure they constitute 30% of each batch \n",
    "from dragonn.generators import * \n",
    "spi1_train_classification_gen=DataGenerator(\"SPI1.train.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3, batch_size=256)\n",
    "spi1_valid_classification_gen=DataGenerator(\"SPI1.valid.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SeWRHimGd4D",
    "outputId": "7516c239-52e8-4c19-dafc-ee32c264808c"
   },
   "outputs": [],
   "source": [
    "#Train the SPI1 classification model \n",
    "spi1_kat_classification_model=initialize_kat_classification_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_kat_classification=spi1_kat_classification_model.fit_generator(spi1_train_classification_gen,\n",
    "                                                  validation_data=spi1_valid_classification_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3D2uhXwGd4E",
    "outputId": "b5450d6e-447b-4a4a-e5c0-19890b4949bc"
   },
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "plot_learning_curve(history_kat_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jq6tLmcTGd4G",
    "outputId": "9757df93-a9e1-45c9-85f9-8cc5c51fd926"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Group.__del__ of <closed tables.group.Group at 0x7f1f8528f728>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/annashch/miniconda3/lib/python3.6/site-packages/tables/group.py\", line 271, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spi1_kat_classification_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bd3963f58dad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                          tasks=['SPI1'])\n\u001b[0;32m----> 8\u001b[0;31m spi1_test_classification_predictions=spi1_kat_classification_model.predict_generator(spi1_test_classification_gen,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                                                \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                                \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spi1_kat_classification_model' is not defined"
     ]
    }
   ],
   "source": [
    "from dragonn.generators import * \n",
    "spi1_test_classification_gen=DataGenerator(\"SPI1.test.classification.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_classification_predictions=spi1_kat_classification_model.predict_generator(spi1_test_classification_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_classification_truth=spi1_test_classification_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fWPY4DoGd4I",
    "outputId": "ab351e3b-7f55-465f-8f74-a5e8b7981d18"
   },
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "from dragonn.metrics import ClassificationResult\n",
    "print(ClassificationResult(spi1_test_classification_truth.values.astype(bool),spi1_test_classification_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPWznGgeGd4K"
   },
   "source": [
    "## Kat's Model Architecture (Regression)<a name='b'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Nt_b4_BGd4K"
   },
   "outputs": [],
   "source": [
    "def initialize_kat_regression_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(ntasks))\n",
    "\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wm8P3ABcGd4L"
   },
   "outputs": [],
   "source": [
    "#create the generators, no upsampling of positives is used for regression. \n",
    "from dragonn.generators import * \n",
    "spi1_train_regression_gen=DataGenerator(\"SPI1.train.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.01)\n",
    "spi1_valid_regression_gen=DataGenerator(\"SPI1.valid.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O2xnIWHXGd4M",
    "outputId": "716c5c00-0b5b-417d-aa33-72cd483c7546"
   },
   "outputs": [],
   "source": [
    "#Train the SPI1 regression model \n",
    "spi1_kat_regression_model=initialize_kat_regression_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_kat_regression=spi1_kat_regression_model.fit_generator(spi1_train_regression_gen,\n",
    "                                                  validation_data=spi1_valid_regression_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2sQWdgtGd4O",
    "outputId": "3db4acbc-8de5-4d24-96da-3c4c41eac26a"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(history_kat_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvSYpEFZGd4P",
    "outputId": "7fcf68b2-7f0e-4191-ae45-71948da1cd3f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spi1_kat_regression_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1d9ba8cf013b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                          tasks=['SPI1'])\n\u001b[0;32m----> 8\u001b[0;31m spi1_test_regression_predictions=spi1_kat_regression_model.predict_generator(spi1_test_regression_gen,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                                                \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                                \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spi1_kat_regression_model' is not defined"
     ]
    }
   ],
   "source": [
    "from dragonn.generators import * \n",
    "spi1_test_regression_gen=DataGenerator(\"SPI1.test.regression.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_regression_predictions=spi1_kat_regression_model.predict_generator(spi1_test_regression_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_regression_truth=spi1_test_regression_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vIyYceoGd4R"
   },
   "outputs": [],
   "source": [
    "## find the indices of the non-zero coverage bins \n",
    "nonzero_bins=spi1_test_regression_truth.max(axis=1)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJ8NbtgWGd4T",
    "outputId": "8a0c8f6c-512f-419e-dd49-8002895cdeb7"
   },
   "outputs": [],
   "source": [
    "#Calculate spearman and pearson correlation between truth labels and predictions \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "corr_pearson=pearsonr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "corr_spearman=spearmanr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpnVA7VsGd4U",
    "outputId": "94a59775-862e-48e4-f036-d7d3fb4d7654"
   },
   "outputs": [],
   "source": [
    "# Calculate the spearman and pearson correlation, restricted to non-zero bins \n",
    "corr_pearson_nonzero_bins=pearsonr(spi1_test_regression_truth[nonzero_bins],spi1_test_regression_predictions[nonzero_bins])\n",
    "corr_spearman_nonzero_bins=spearmanr(spi1_test_regression_truth[nonzero_bins],spi1_test_regression_predictions[nonzero_bins])\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson_nonzero_bins))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman_nonzero_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCaEkJICGd4V"
   },
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame({\"Observed\":list(spi1_test_regression_truth.values.squeeze()),\n",
    "                     \"Predicted\":list(spi1_test_regression_predictions.squeeze())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kktZbRCGd4W"
   },
   "outputs": [],
   "source": [
    "test_df_nonzero=pd.DataFrame({\"Observed\":list(spi1_test_regression_truth[nonzero_bins].values.squeeze()),\n",
    "                     \"Predicted\":list(spi1_test_regression_predictions[nonzero_bins].squeeze())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "af-4fxu0Gd4X",
    "outputId": "bc675856-7680-4a00-d016-a4e0fe3e6d55"
   },
   "outputs": [],
   "source": [
    "import plotnine \n",
    "from plotnine import * \n",
    "print((ggplot(test_df,aes(x=\"Observed\",y=\"Predicted\"))\n",
    " +geom_bin2d(bins=100)\n",
    " +theme_bw()\n",
    " +xlab(\"Observed asinh(mean coverage in FC bigWig\")\n",
    " +ylab(\"Model prediction\")\n",
    " +ggtitle(\"SPI1 regression model test set prediction\")))\n",
    "\n",
    "print((ggplot(test_df_nonzero,aes(x=\"Observed\",y=\"Predicted\"))\n",
    " +geom_bin2d(bins=100)\n",
    " +theme_bw()\n",
    " +xlab(\"Observed asinh(mean coverage in FC bigWig\")\n",
    " +ylab(\"Model prediction\")\n",
    " +ggtitle(\"SPI1 regression model test set prediction: bins with nonzero coverage\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZnRq3SzGd4Z"
   },
   "source": [
    "## Kat's Model DeepLIFT profiles (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdWiImsYGd4Z"
   },
   "outputs": [],
   "source": [
    "spi1_test_classification_truth_bool=spi1_test_classification_truth.values.astype(bool)\n",
    "true_pos_spi1=spi1_test_classification_truth[spi1_test_classification_truth_bool*spi1_test_classification_predictions >0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iSq-J-iGd4a",
    "outputId": "578fdbcd-8cd1-4d5d-c6e2-4d28234b6610"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import one_hot_from_bed\n",
    "deep_lift_input_spi1=one_hot_from_bed([i for i in true_pos_spi1.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRIkb8oFGd4c"
   },
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import deeplift, plot_seq_importance\n",
    "deep_lift_scores_spi1=deeplift(spi1_kat_classification_model,deep_lift_input_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCAY5G5JGd4g",
    "outputId": "580aca0a-9a35-4cc8-84bc-15e9bf399d56"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])\n",
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])\n",
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fefAyEp4Gd4h",
    "outputId": "dca31e18-7f19-48bd-e76d-86a98e6cb221"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[400:500],deep_lift_input_spi1[2].squeeze()[400:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaizmyYbGd4i"
   },
   "source": [
    "If we query the sequence \"GTTTCACTTCTGCAAA\" in the [TomTom](http://meme-suite.org/tools/tomtom) software from the MEME suite, we find that the motif is a good match (p=3.55e-03) for SPIB: \n",
    "<img src=\"https://github.com/kundajelab/dragonn/blob/master/paper_supplement/tutorial_images/SPIB.Kat.png?raw=1\" alt=\"SPI12TomTom\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFlubWfKGd4i"
   },
   "source": [
    "## Kat's Model DeepLIFT profiles (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DdIebdVGd4j",
    "outputId": "450f7622-0471-49af-fcb4-cd442ec04c57"
   },
   "outputs": [],
   "source": [
    "#Sanity-check that the model is learning the SPI1 motif by running DeepLIFT on True Positives with high confidence (>0.9)\n",
    "#get the true positive predictions \n",
    "true_pos=spi1_test_regression_truth[(spi1_test_regression_truth.values*spi1_test_regression_predictions)>4]\n",
    "true_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "My8t2AvgGd4k",
    "outputId": "65d55c2a-48ca-4878-f554-5dcf00e33c9f"
   },
   "outputs": [],
   "source": [
    "deep_lift_input=one_hot_from_bed([i for i in true_pos.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzURc6r9Gd4l"
   },
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1=deeplift(spi1_regression_model,deep_lift_input_spi1,target_layer_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqrN9urUGd4n",
    "outputId": "709f3bef-fe1b-40d3-b775-d20740a84deb"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])\n",
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])\n",
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leSwvZWFGd4n",
    "outputId": "848dd89f-eaa3-4872-ae04-3301ac0fff7d"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[400:500],deep_lift_input_spi1[2].squeeze()[400:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMBWHvnZV42V"
   },
   "source": [
    "## Conclusions <a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFyKUGQnV42X"
   },
   "source": [
    "## Save tutorial outputs <a name='10'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "We save the models and test set predictions generated in this tutorial to an hdf5 file so that they can be loaded more readily in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBNQNhg3V42X"
   },
   "outputs": [],
   "source": [
    "#save the models \n",
    "#spi1_kat_classification_model.save(\"SPI1.kat.classification.model.hdf5\")\n",
    "#spi1_kat_regression_model.save(\"SPI1.kat.regression.model.hdf5\")\n",
    "\n",
    "#spi1_classification_model.save(\"SPI1.classification.model.hdf5\")\n",
    "#spi1_regression_model.save(\"SPI1.regression.model.hdf5\")\n",
    "#save the test predictions \n",
    "import h5py \n",
    "test_set_predictions=h5py.File(\"SPI1.test.kat.predictions.hdf5\",'w')\n",
    "test_set_predictions.create_dataset(\"classification\",data=spi1_test_classification_predictions)\n",
    "test_set_predictions.create_dataset(\"regression\",data=spi1_test_regression_predictions)\n",
    "test_set_predictions.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOPcHHY5V42a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "PrimerTutorial 5 - Functional variant characterization for non-coding SNPs within the SPI1 motif.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
