import argparsefrom elisa_clean.utils import *if __name__ == '__main__':    parser = argparse.ArgumentParser(description='Clean the lexicon')    parser.add_argument('--remove_all', dest='remove_all', default=False, action='store_true',                        help='remove all non-splitting punctuations, modifier pronouns')    parser.add_argument('--remove_open', dest='remove_open', default=False, action='store_true',                        help='remove all open brackets')    parser.add_argument('--remove_punct', dest='remove_punct', default=False, action='store_true',                        help='remove all non-splitting punctuations')    parser.add_argument('--remove_pron', dest='remove_pron', default=False, action='store_true',                        help='remove all modifier pronouns')    parser.add_argument('--remove_abbrev', dest='remove_abbrev', default=False, action='store_true',                        help='remove all abbrevations')    parser.add_argument('--filter_all', dest='filter_all', default=False, action='store_true',                        help='extract tokens that have the same pos or are entities')    parser.add_argument('--filter_pos', dest='filter_pos', default=False, action='store_true',                        help='extract tokens by the pos tag')    parser.add_argument('--filter_ent', dest='filter_ent', default=False, action='store_true',                        help='extract entity tokens')    parser.add_argument('--filter_mod', dest='filter_mod', choices=('union', 'inter'),                        help='mode for filtering, either union or intersection')    parser.add_argument('--spacy_model', type=str, choices=('en_core_web_lg', 'en_core_web_md'),                        help='spacy model for pos and entity extraction')    parser.add_argument('--split', dest='split', default=False, action='store_true')    parser.add_argument('format', type=str, help='word, (pos), meaning indexes. e.g 0,1,2 or 0,2')    parser.add_argument('input', type=str, help='input lexicon file')    parser.add_argument('output', type=str, help='output lexicon file')    args = parser.parse_args()    print(args)    nlp = spacy.load(args.spacy_model) if args.spacy_model else None    indexes = list(map(lambda x: int(x.strip()), args.format.split(',')))    with open(args.input) as i, open(args.output, "w") as o:        for line in i.readlines():            tokens = line.strip('\n').split('\t')            pos = None            if len(indexes) == 3:                word, pos, meaning = [tokens[i] for i in indexes]            elif len(indexes) == 2:                word, meaning = [tokens[i] for i in indexes]            else:                raise Exception('wrong indexes')            if args.remove_all or args.remove_open:                meaning = remove_open_punctuations(meaning)            if args.remove_all or args.remove_abbrev:                meaning = remove_abbrev(meaning)            if args.remove_all or args.remove_punct:                meaning = remove_punctuations(meaning)            if args.remove_all or args.remove_pron:                meaning = remove_pronouns(meaning)            if args.split:                meanings = split(meaning)            for meaning in meanings:                if pos is not None and (args.filter_all or args.filter_pos):                    meaning_pos = filter_pos(meaning, pos, nlp)  # Only POS-matched tokens and symbols are kept                if args.filter_all or args.filter_ent:                    meaning_ent = filter_entity(meaning, nlp)  # Only entities and symbols are kept                if args.filter_mod:                    assert meaning_ent is not None and meaning_pos is not None, "Wrong output from filter functions"                    if args.filter_mod == 'union':                        meaning_tokens = []                        for t in meaning_pos.split():                            if t in meaning_ent:                                meaning_tokens.append(t)                        meaning = ' '.join(meaning_tokens)                    else:                        tokens = []                        for t in nlp(meaning):                            if t.text in meaning_pos or t.text in meaning_ent:                                meaning_tokens.append(t.text)                        meaning = ' '.join(meaning_tokens)                if meaning.strip():                    o.write('\t'.join(x.strip() if i != indexes[-1] else meaning.strip() for i, x in enumerate(tokens)) + '\n')