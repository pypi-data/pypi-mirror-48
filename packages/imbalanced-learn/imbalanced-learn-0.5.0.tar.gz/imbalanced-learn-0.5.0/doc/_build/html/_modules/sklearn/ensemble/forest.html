

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.ensemble.forest &mdash; imbalanced-learn 0.5.0.dev0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/imbalanced-learn.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/gallery.css" type="text/css" />
    <link rel="author" title="About these documents" href="../../../about.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> imbalanced-learn
          

          
          </a>

          
            
            
              <div class="version">
                0.5.0.dev0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install and contribution</a></li>
</ul>
<p class="caption"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">imbalanced-learn API</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorial - Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html">General examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html#examples-based-on-real-world-datasets">Examples based on real world datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html#dataset-examples">Dataset examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html#evaluation-examples">Evaluation examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html#model-selection">Model Selection</a></li>
</ul>
<p class="caption"><span class="caption-text">Addtional Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../whats_new.html">Release history</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about.html">About us</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">imbalanced-learn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>sklearn.ensemble.forest</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for sklearn.ensemble.forest</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Forest of trees-based ensemble methods</span>

<span class="sd">Those methods include random forests and extremely randomized trees.</span>

<span class="sd">The module structure is the following:</span>

<span class="sd">- The ``BaseForest`` base class implements a common ``fit`` method for all</span>
<span class="sd">  the estimators in the module. The ``fit`` method of the base ``Forest``</span>
<span class="sd">  class calls the ``fit`` method of each sub-estimator on random samples</span>
<span class="sd">  (with replacement, a.k.a. bootstrap) of the training set.</span>

<span class="sd">  The init of the sub-estimator is further delegated to the</span>
<span class="sd">  ``BaseEnsemble`` constructor.</span>

<span class="sd">- The ``ForestClassifier`` and ``ForestRegressor`` base classes further</span>
<span class="sd">  implement the prediction logic by computing an average of the predicted</span>
<span class="sd">  outcomes of the sub-estimators.</span>

<span class="sd">- The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived</span>
<span class="sd">  classes provide the user with concrete implementations of</span>
<span class="sd">  the forest ensemble method using classical, deterministic</span>
<span class="sd">  ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as</span>
<span class="sd">  sub-estimator implementations.</span>

<span class="sd">- The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived</span>
<span class="sd">  classes provide the user with concrete implementations of the</span>
<span class="sd">  forest ensemble method using the extremely randomized trees</span>
<span class="sd">  ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as</span>
<span class="sd">  sub-estimator implementations.</span>

<span class="sd">Single and multi-output problems are both handled.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Gilles Louppe &lt;g.louppe@gmail.com&gt;</span>
<span class="c1">#          Brian Holt &lt;bdholt1@gmail.com&gt;</span>
<span class="c1">#          Joly Arnaud &lt;arnaud.v.joly@gmail.com&gt;</span>
<span class="c1">#          Fares Hedayati &lt;fares.hedayati@gmail.com&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>

<span class="kn">from</span> <span class="nn">warnings</span> <span class="k">import</span> <span class="n">catch_warnings</span><span class="p">,</span> <span class="n">simplefilter</span><span class="p">,</span> <span class="n">warn</span>
<span class="kn">import</span> <span class="nn">threading</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="k">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="k">import</span> <span class="n">issparse</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="k">import</span> <span class="n">hstack</span> <span class="k">as</span> <span class="n">sparse_hstack</span>


<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">..externals</span> <span class="k">import</span> <span class="n">six</span>
<span class="kn">from</span> <span class="nn">..metrics</span> <span class="k">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">..preprocessing</span> <span class="k">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">..tree</span> <span class="k">import</span> <span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span>
                    <span class="n">ExtraTreeClassifier</span><span class="p">,</span> <span class="n">ExtraTreeRegressor</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">..tree._tree</span> <span class="k">import</span> <span class="n">DTYPE</span><span class="p">,</span> <span class="n">DOUBLE</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">compute_sample_weight</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="k">import</span> <span class="n">DataConversionWarning</span><span class="p">,</span> <span class="n">NotFittedError</span>
<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">BaseEnsemble</span><span class="p">,</span> <span class="n">_partition_estimators</span>
<span class="kn">from</span> <span class="nn">..utils.fixes</span> <span class="k">import</span> <span class="n">parallel_helper</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="k">import</span> <span class="n">check_classification_targets</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="k">import</span> <span class="n">check_is_fitted</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;RandomForestClassifier&quot;</span><span class="p">,</span>
           <span class="s2">&quot;RandomForestRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;ExtraTreesClassifier&quot;</span><span class="p">,</span>
           <span class="s2">&quot;ExtraTreesRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;RandomTreesEmbedding&quot;</span><span class="p">]</span>

<span class="n">MAX_INT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>


<span class="k">def</span> <span class="nf">_generate_sample_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private function used to _parallel_build_trees function.&quot;&quot;&quot;</span>
    <span class="n">random_instance</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">random_instance</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sample_indices</span>


<span class="k">def</span> <span class="nf">_generate_unsampled_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private function used to forest._set_oob_score function.&quot;&quot;&quot;</span>
    <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">_generate_sample_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">sample_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">sample_indices</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">unsampled_mask</span> <span class="o">=</span> <span class="n">sample_counts</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">indices_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">unsampled_indices</span> <span class="o">=</span> <span class="n">indices_range</span><span class="p">[</span><span class="n">unsampled_mask</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">unsampled_indices</span>


<span class="k">def</span> <span class="nf">_parallel_build_trees</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">forest</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">n_trees</span><span class="p">,</span>
                          <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private function used to fit a single tree in parallel.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;building tree </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tree_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_trees</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">forest</span><span class="o">.</span><span class="n">bootstrap</span><span class="p">:</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">curr_sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">curr_sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">_generate_sample_indices</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">sample_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">sample_counts</span>

        <span class="k">if</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s1">&#39;subsample&#39;</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">catch_warnings</span><span class="p">():</span>
                <span class="n">simplefilter</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
                <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s1">&#39;balanced_subsample&#39;</span><span class="p">:</span>
            <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

        <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">curr_sample_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tree</span>


<span class="k">class</span> <span class="nc">BaseForest</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseEnsemble</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for forests of trees.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BaseForest</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="o">=</span> <span class="n">bootstrap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span> <span class="o">=</span> <span class="n">oob_score</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply trees in the forest to X, return leaf indices.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_leaves : array_like, shape = [n_samples, n_estimators]</span>
<span class="sd">            For each datapoint x in X and for each tree in the forest,</span>
<span class="sd">            return the index of the leaf x ends up in.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                           <span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">parallel_helper</span><span class="p">)(</span><span class="n">tree</span><span class="p">,</span> <span class="s1">&#39;apply&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">decision_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the decision path in the forest</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        indicator : sparse csr array, shape = [n_samples, n_nodes]</span>
<span class="sd">            Return a node indicator matrix where non zero elements</span>
<span class="sd">            indicates that the samples goes through the nodes.</span>

<span class="sd">        n_nodes_ptr : array of size (n_estimators + 1, )</span>
<span class="sd">            The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]</span>
<span class="sd">            gives the indicator value for the i-th estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">indicators</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                              <span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">parallel_helper</span><span class="p">)(</span><span class="n">tree</span><span class="p">,</span> <span class="s1">&#39;decision_path&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span>
                                     <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="n">n_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indicators</span><span class="p">])</span>
        <span class="n">n_nodes_ptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">sparse_hstack</span><span class="p">(</span><span class="n">indicators</span><span class="p">)</span><span class="o">.</span><span class="n">tocsr</span><span class="p">(),</span> <span class="n">n_nodes_ptr</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build a forest of trees from the training set (X, y).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Internally, its dtype will be converted</span>
<span class="sd">            to ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csc_matrix``.</span>

<span class="sd">        y : array-like, shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">==</span> <span class="s1">&#39;warn&#39;</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The default value of n_estimators will change from &quot;</span>
                          <span class="s2">&quot;10 in version 0.20 to 100 in 0.22.&quot;</span><span class="p">,</span> <span class="ne">FutureWarning</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">10</span>

        <span class="c1"># Validate or convert input data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Pre-sort indices to avoid that each individual tree of the</span>
            <span class="c1"># ensemble sorts the indices.</span>
            <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

        <span class="c1"># Remap output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;A column-vector y was passed when a 1d array was&quot;</span>
                 <span class="s2">&quot; expected. Please change the shape of y to &quot;</span>
                 <span class="s2">&quot;(n_samples,), for example using ravel().&quot;</span><span class="p">,</span>
                 <span class="n">DataConversionWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># reshape is necessary to preserve the data contiguity against vs</span>
            <span class="c1"># [:, np.newaxis] that does not.</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y_class_weight</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">y</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">expanded_class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">expanded_class_weight</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expanded_class_weight</span>

        <span class="c1"># Check parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Out of bag estimation only available&quot;</span>
                             <span class="s2">&quot; if bootstrap=True&quot;</span><span class="p">)</span>

        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimators_&quot;</span><span class="p">):</span>
            <span class="c1"># Free allocated memory, if any</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">n_more_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_more_estimators</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;n_estimators=</span><span class="si">%d</span><span class="s1"> must be larger or equal to &#39;</span>
                             <span class="s1">&#39;len(estimators_)=</span><span class="si">%d</span><span class="s1"> when warm_start==True&#39;</span>
                             <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)))</span>

        <span class="k">elif</span> <span class="n">n_more_estimators</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
                 <span class="s2">&quot;fit new trees.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># We draw from the random state to get the random state we</span>
                <span class="c1"># would have got if we hadn&#39;t used a warm_start.</span>
                <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>

            <span class="n">trees</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_more_estimators</span><span class="p">):</span>
                <span class="n">tree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">append</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
                <span class="n">trees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>

            <span class="c1"># Parallel loop: we prefer the threading backend as the Cython code</span>
            <span class="c1"># for fitting the trees is internally releasing the Python GIL</span>
            <span class="c1"># making threading more efficient than multiprocessing in</span>
            <span class="c1"># that case. However, we respect any parallel_backend contexts set</span>
            <span class="c1"># at a higher level, since correctness does not rely on using</span>
            <span class="c1"># threads.</span>
            <span class="n">trees</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                             <span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">)(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_build_trees</span><span class="p">)(</span>
                    <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">),</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trees</span><span class="p">))</span>

            <span class="c1"># Collect newly grown trees</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_oob_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Decapsulate classes_ attributes</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;classes_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculate out of bag predictions and score.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_validate_y_class_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Default implementation</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_validate_X_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Validate X whenever one tries to predict, apply, predict_proba&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">NotFittedError</span><span class="p">(</span><span class="s2">&quot;Estimator not fitted, &quot;</span>
                                 <span class="s2">&quot;call `fit` before exploiting the model.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">feature_importances_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the feature importances (the higher, the more important the</span>
<span class="sd">           feature).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        feature_importances_ : array, shape = [n_features]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>

        <span class="n">all_importances</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span>
                                   <span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="nb">getattr</span><span class="p">)(</span><span class="n">tree</span><span class="p">,</span> <span class="s1">&#39;feature_importances_&#39;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">all_importances</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_accumulate_prediction</span><span class="p">(</span><span class="n">predict</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">lock</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This is a utility function for joblib&#39;s Parallel.</span>

<span class="sd">    It can&#39;t go locally in ForestClassifier or ForestRegressor, because joblib</span>
<span class="sd">    complains that it cannot pickle it when placed there.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">lock</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">prediction</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)):</span>
                <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">ForestClassifier</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseForest</span><span class="p">,</span>
                                          <span class="n">ClassifierMixin</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for forest of trees-based classifiers.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ForestClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute out-of-bag score&quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>

        <span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">oob_decision_function</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">oob_score</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes_</span><span class="p">[</span><span class="n">k</span><span class="p">])))</span>

        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">unsampled_indices</span> <span class="o">=</span> <span class="n">_generate_unsampled_indices</span><span class="p">(</span>
                <span class="n">estimator</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">p_estimator</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:],</span>
                                                  <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">p_estimator</span> <span class="o">=</span> <span class="p">[</span><span class="n">p_estimator</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
                <span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">p_estimator</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Some inputs do not have OOB scores. &quot;</span>
                     <span class="s2">&quot;This probably means too few trees were used &quot;</span>
                     <span class="s2">&quot;to compute any reliable oob estimates.&quot;</span><span class="p">)</span>

            <span class="n">decision</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">/</span>
                        <span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
            <span class="n">oob_decision_function</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decision</span><span class="p">)</span>
            <span class="n">oob_score</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">==</span>
                                 <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_decision_function_</span> <span class="o">=</span> <span class="n">oob_decision_function</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_decision_function_</span> <span class="o">=</span> <span class="n">oob_decision_function</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">=</span> <span class="n">oob_score</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span>

    <span class="k">def</span> <span class="nf">_validate_y_class_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">y_store_unique_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="n">classes_k</span><span class="p">,</span> <span class="n">y_store_unique_indices</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classes_k</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classes_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y_store_unique_indices</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">valid_presets</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span> <span class="s1">&#39;balanced_subsample&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_presets</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Valid presets for class_weight include &#39;</span>
                                     <span class="s1">&#39;&quot;balanced&quot; and &quot;balanced_subsample&quot;. Given &quot;</span><span class="si">%s</span><span class="s1">&quot;.&#39;</span>
                                     <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
                    <span class="n">warn</span><span class="p">(</span><span class="s1">&#39;class_weight presets &quot;balanced&quot; or &quot;balanced_subsample&quot; are &#39;</span>
                         <span class="s1">&#39;not recommended for warm_start if the fitted data &#39;</span>
                         <span class="s1">&#39;differs from the full dataset. In order to use &#39;</span>
                         <span class="s1">&#39;&quot;balanced&quot; weights, use compute_class_weight(&quot;balanced&quot;, &#39;</span>
                         <span class="s1">&#39;classes, y). In place of y you can use a large &#39;</span>
                         <span class="s1">&#39;enough sample of the full training set target to &#39;</span>
                         <span class="s1">&#39;properly estimate the class frequency &#39;</span>
                         <span class="s1">&#39;distributions. Pass the resulting weights as the &#39;</span>
                         <span class="s1">&#39;class_weight parameter.&#39;</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">!=</span> <span class="s1">&#39;balanced_subsample&#39;</span> <span class="ow">or</span>
                    <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;balanced_subsample&quot;</span><span class="p">:</span>
                    <span class="n">class_weight</span> <span class="o">=</span> <span class="s2">&quot;balanced&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span>
                <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span>
                                                              <span class="n">y_original</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class for X.</span>

<span class="sd">        The predicted class of an input sample is a vote by the trees in</span>
<span class="sd">        the forest, weighted by their probability estimates. That is,</span>
<span class="sd">        the predicted class is the one with highest mean probability</span>
<span class="sd">        estimate across the trees.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The predicted classes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">proba</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
                <span class="n">predictions</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                                                                    <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                                          <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">predictions</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>

<span class="sd">        The predicted class probabilities of an input sample are computed as</span>
<span class="sd">        the mean predicted class probabilities of the trees in the forest. The</span>
<span class="sd">        class probability of a single tree is the fraction of samples of the same</span>
<span class="sd">        class in a leaf.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : array of shape = [n_samples, n_classes], or a list of n_outputs</span>
<span class="sd">            such arrays if n_outputs &gt; 1.</span>
<span class="sd">            The class probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute `classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="c1"># Check data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Assign chunk of trees to jobs</span>
        <span class="n">n_jobs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_partition_estimators</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span>

        <span class="c1"># avoid storing the output of every estimator by summing them here</span>
        <span class="n">all_proba</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">j</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">)]</span>
        <span class="n">lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
        <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">require</span><span class="o">=</span><span class="s2">&quot;sharedmem&quot;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_accumulate_prediction</span><span class="p">)(</span><span class="n">e</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">all_proba</span><span class="p">,</span>
                                            <span class="n">lock</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">proba</span> <span class="ow">in</span> <span class="n">all_proba</span><span class="p">:</span>
            <span class="n">proba</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_proba</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_proba</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_proba</span>

    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class log-probabilities for X.</span>

<span class="sd">        The predicted class log-probabilities of an input sample is computed as</span>
<span class="sd">        the log of the mean predicted class probabilities of the trees in the</span>
<span class="sd">        forest.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : array of shape = [n_samples, n_classes], or a list of n_outputs</span>
<span class="sd">            such arrays if n_outputs &gt; 1.</span>
<span class="sd">            The class probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute `classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
                <span class="n">proba</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

            <span class="k">return</span> <span class="n">proba</span>


<span class="k">class</span> <span class="nc">ForestRegressor</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseForest</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for forest of trees-based regressors.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target for X.</span>

<span class="sd">        The predicted regression target of an input sample is computed as the</span>
<span class="sd">        mean predicted regression targets of the trees in the forest.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="c1"># Check data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Assign chunk of trees to jobs</span>
        <span class="n">n_jobs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_partition_estimators</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span>

        <span class="c1"># avoid storing the output of every estimator by summing them here</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="c1"># Parallel loop</span>
        <span class="n">lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
        <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">require</span><span class="o">=</span><span class="s2">&quot;sharedmem&quot;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_accumulate_prediction</span><span class="p">)(</span><span class="n">e</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="n">y_hat</span><span class="p">],</span> <span class="n">lock</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="n">y_hat</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_hat</span>

    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute out-of-bag scores&quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">))</span>
        <span class="n">n_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">unsampled_indices</span> <span class="o">=</span> <span class="n">_generate_unsampled_indices</span><span class="p">(</span>
                <span class="n">estimator</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">p_estimator</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:],</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">p_estimator</span> <span class="o">=</span> <span class="n">p_estimator</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

            <span class="n">predictions</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">p_estimator</span>
            <span class="n">n_predictions</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">n_predictions</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Some inputs do not have OOB scores. &quot;</span>
                 <span class="s2">&quot;This probably means too few trees were used &quot;</span>
                 <span class="s2">&quot;to compute any reliable oob estimates.&quot;</span><span class="p">)</span>
            <span class="n">n_predictions</span><span class="p">[</span><span class="n">n_predictions</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">predictions</span> <span class="o">/=</span> <span class="n">n_predictions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span> <span class="o">=</span> <span class="n">predictions</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">+=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span>
                                        <span class="n">predictions</span><span class="p">[:,</span> <span class="n">k</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span>


<span class="k">class</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">ForestClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A random forest classifier.</span>

<span class="sd">    A random forest is a meta estimator that fits a number of decision tree</span>
<span class="sd">    classifiers on various sub-samples of the dataset and uses averaging to</span>
<span class="sd">    improve the predictive accuracy and control over-fitting.</span>
<span class="sd">    The sub-sample size is always the same as the original</span>
<span class="sd">    input sample size but the samples are drawn with replacement if</span>
<span class="sd">    `bootstrap=True` (default).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;forest&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : integer, optional (default=10)</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.20</span>
<span class="sd">           The default value of ``n_estimators`` will change from 10 in</span>
<span class="sd">           version 0.20 to 100 in version 0.22.</span>

<span class="sd">    criterion : string, optional (default=&quot;gini&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria are</span>
<span class="sd">        &quot;gini&quot; for the Gini impurity and &quot;entropy&quot; for the information gain.</span>
<span class="sd">        Note: this parameter is tree-specific.</span>

<span class="sd">    max_depth : integer or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=&quot;auto&quot;)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)` (same as &quot;auto&quot;).</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float,</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    bootstrap : boolean, optional (default=True)</span>
<span class="sd">        Whether bootstrap samples are used when building trees.</span>

<span class="sd">    oob_score : bool (default=False)</span>
<span class="sd">        Whether to use out-of-bag samples to estimate</span>
<span class="sd">        the generalization accuracy.</span>

<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    class_weight : dict, list of dicts, &quot;balanced&quot;, &quot;balanced_subsample&quot; or \</span>
<span class="sd">    None, optional (default=None)</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one. For</span>
<span class="sd">        multi-output problems, a list of dicts can be provided in the same</span>
<span class="sd">        order as the columns of y.</span>

<span class="sd">        Note that for multioutput (including multilabel) weights should be</span>
<span class="sd">        defined for each class of every column in its own dict. For example,</span>
<span class="sd">        for four-class multilabel classification weights should be</span>
<span class="sd">        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of</span>
<span class="sd">        [{1:1}, {2:5}, {3:1}, {4:1}].</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">        The &quot;balanced_subsample&quot; mode is the same as &quot;balanced&quot; except that</span>
<span class="sd">        weights are computed based on the bootstrap sample for every tree</span>
<span class="sd">        grown.</span>

<span class="sd">        For multi-output, the weights of each column of y will be multiplied.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeClassifier</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    classes_ : array of shape = [n_classes] or a list of such arrays</span>
<span class="sd">        The classes labels (single output problem), or a list of arrays of</span>
<span class="sd">        class labels (multi-output problem).</span>

<span class="sd">    n_classes_ : int or list</span>
<span class="sd">        The number of classes (single output problem), or a list containing the</span>
<span class="sd">        number of classes for each output (multi-output problem).</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>

<span class="sd">    oob_decision_function_ : array of shape = [n_samples, n_classes]</span>
<span class="sd">        Decision function computed with out-of-bag estimate on the training</span>
<span class="sd">        set. If n_estimators is small it might be possible that a data point</span>
<span class="sd">        was never left out during the bootstrap. In this case,</span>
<span class="sd">        `oob_decision_function_` might contain NaN.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_classification</span>

<span class="sd">    &gt;&gt;&gt; X, y = make_classification(n_samples=1000, n_features=4,</span>
<span class="sd">    ...                            n_informative=2, n_redundant=0,</span>
<span class="sd">    ...                            random_state=0, shuffle=False)</span>
<span class="sd">    &gt;&gt;&gt; clf = RandomForestClassifier(n_estimators=100, max_depth=2,</span>
<span class="sd">    ...                              random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y)</span>
<span class="sd">    RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,</span>
<span class="sd">                max_depth=2, max_features=&#39;auto&#39;, max_leaf_nodes=None,</span>
<span class="sd">                min_impurity_decrease=0.0, min_impurity_split=None,</span>
<span class="sd">                min_samples_leaf=1, min_samples_split=2,</span>
<span class="sd">                min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,</span>
<span class="sd">                oob_score=False, random_state=0, verbose=0, warm_start=False)</span>
<span class="sd">    &gt;&gt;&gt; print(clf.feature_importances_)</span>
<span class="sd">    [0.14205973 0.76664038 0.0282433  0.06305659]</span>
<span class="sd">    &gt;&gt;&gt; print(clf.predict([[0, 0, 0, 0]]))</span>
<span class="sd">    [1]</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>

<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data,</span>
<span class="sd">    ``max_features=n_features`` and ``bootstrap=False``, if the improvement</span>
<span class="sd">    of the criterion is identical for several splits enumerated during the</span>
<span class="sd">    search of the best split. To obtain a deterministic behaviour during</span>
<span class="sd">    fitting, ``random_state`` has to be fixed.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] L. Breiman, &quot;Random Forests&quot;, Machine Learning, 45(1), 5-32, 2001.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    DecisionTreeClassifier, ExtraTreesClassifier</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;gini&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>


<span class="k">class</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">ForestRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A random forest regressor.</span>

<span class="sd">    A random forest is a meta estimator that fits a number of classifying</span>
<span class="sd">    decision trees on various sub-samples of the dataset and uses averaging</span>
<span class="sd">    to improve the predictive accuracy and control over-fitting.</span>
<span class="sd">    The sub-sample size is always the same as the original</span>
<span class="sd">    input sample size but the samples are drawn with replacement if</span>
<span class="sd">    `bootstrap=True` (default).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;forest&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : integer, optional (default=10)</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.20</span>
<span class="sd">           The default value of ``n_estimators`` will change from 10 in</span>
<span class="sd">           version 0.20 to 100 in version 0.22.</span>

<span class="sd">    criterion : string, optional (default=&quot;mse&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;mse&quot; for the mean squared error, which is equal to variance</span>
<span class="sd">        reduction as feature selection criterion, and &quot;mae&quot; for the mean</span>
<span class="sd">        absolute error.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Mean Absolute Error (MAE) criterion.</span>

<span class="sd">    max_depth : integer or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=&quot;auto&quot;)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float,</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    bootstrap : boolean, optional (default=True)</span>
<span class="sd">        Whether bootstrap samples are used when building trees.</span>

<span class="sd">    oob_score : bool, optional (default=False)</span>
<span class="sd">        whether to use out-of-bag samples to estimate</span>
<span class="sd">        the R^2 on unseen data.</span>

<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeRegressor</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>

<span class="sd">    oob_prediction_ : array of shape = [n_samples]</span>
<span class="sd">        Prediction computed with out-of-bag estimate on the training set.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_regression</span>

<span class="sd">    &gt;&gt;&gt; X, y = make_regression(n_features=4, n_informative=2,</span>
<span class="sd">    ...                        random_state=0, shuffle=False)</span>
<span class="sd">    &gt;&gt;&gt; regr = RandomForestRegressor(max_depth=2, random_state=0,</span>
<span class="sd">    ...                              n_estimators=100)</span>
<span class="sd">    &gt;&gt;&gt; regr.fit(X, y)</span>
<span class="sd">    RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=2,</span>
<span class="sd">               max_features=&#39;auto&#39;, max_leaf_nodes=None,</span>
<span class="sd">               min_impurity_decrease=0.0, min_impurity_split=None,</span>
<span class="sd">               min_samples_leaf=1, min_samples_split=2,</span>
<span class="sd">               min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,</span>
<span class="sd">               oob_score=False, random_state=0, verbose=0, warm_start=False)</span>
<span class="sd">    &gt;&gt;&gt; print(regr.feature_importances_)</span>
<span class="sd">    [0.18146984 0.81473937 0.00145312 0.00233767]</span>
<span class="sd">    &gt;&gt;&gt; print(regr.predict([[0, 0, 0, 0]]))</span>
<span class="sd">    [-8.32987858]</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>

<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data,</span>
<span class="sd">    ``max_features=n_features`` and ``bootstrap=False``, if the improvement</span>
<span class="sd">    of the criterion is identical for several splits enumerated during the</span>
<span class="sd">    search of the best split. To obtain a deterministic behaviour during</span>
<span class="sd">    fitting, ``random_state`` has to be fixed.</span>

<span class="sd">    The default value ``max_features=&quot;auto&quot;`` uses ``n_features`` </span>
<span class="sd">    rather than ``n_features / 3``. The latter was originally suggested in</span>
<span class="sd">    [1], whereas the former was more recently justified empirically in [2].</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] L. Breiman, &quot;Random Forests&quot;, Machine Learning, 45(1), 5-32, 2001.</span>

<span class="sd">    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized </span>
<span class="sd">           trees&quot;, Machine Learning, 63(1), 3-42, 2006.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    DecisionTreeRegressor, ExtraTreesRegressor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">DecisionTreeRegressor</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>


<span class="k">class</span> <span class="nc">ExtraTreesClassifier</span><span class="p">(</span><span class="n">ForestClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An extra-trees classifier.</span>

<span class="sd">    This class implements a meta estimator that fits a number of</span>
<span class="sd">    randomized decision trees (a.k.a. extra-trees) on various sub-samples</span>
<span class="sd">    of the dataset and uses averaging to improve the predictive accuracy</span>
<span class="sd">    and control over-fitting.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;forest&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : integer, optional (default=10)</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.20</span>
<span class="sd">           The default value of ``n_estimators`` will change from 10 in</span>
<span class="sd">           version 0.20 to 100 in version 0.22.</span>

<span class="sd">    criterion : string, optional (default=&quot;gini&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria are</span>
<span class="sd">        &quot;gini&quot; for the Gini impurity and &quot;entropy&quot; for the information gain.</span>

<span class="sd">    max_depth : integer or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=&quot;auto&quot;)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float,</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    bootstrap : boolean, optional (default=False)</span>
<span class="sd">        Whether bootstrap samples are used when building trees.</span>

<span class="sd">    oob_score : bool, optional (default=False)</span>
<span class="sd">        Whether to use out-of-bag samples to estimate</span>
<span class="sd">        the generalization accuracy.</span>

<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    class_weight : dict, list of dicts, &quot;balanced&quot;, &quot;balanced_subsample&quot; or \</span>
<span class="sd">    None, optional (default=None)</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one. For</span>
<span class="sd">        multi-output problems, a list of dicts can be provided in the same</span>
<span class="sd">        order as the columns of y.</span>

<span class="sd">        Note that for multioutput (including multilabel) weights should be</span>
<span class="sd">        defined for each class of every column in its own dict. For example,</span>
<span class="sd">        for four-class multilabel classification weights should be</span>
<span class="sd">        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of</span>
<span class="sd">        [{1:1}, {2:5}, {3:1}, {4:1}].</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">        The &quot;balanced_subsample&quot; mode is the same as &quot;balanced&quot; except that weights are</span>
<span class="sd">        computed based on the bootstrap sample for every tree grown.</span>

<span class="sd">        For multi-output, the weights of each column of y will be multiplied.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeClassifier</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    classes_ : array of shape = [n_classes] or a list of such arrays</span>
<span class="sd">        The classes labels (single output problem), or a list of arrays of</span>
<span class="sd">        class labels (multi-output problem).</span>

<span class="sd">    n_classes_ : int or list</span>
<span class="sd">        The number of classes (single output problem), or a list containing the</span>
<span class="sd">        number of classes for each output (multi-output problem).</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>

<span class="sd">    oob_decision_function_ : array of shape = [n_samples, n_classes]</span>
<span class="sd">        Decision function computed with out-of-bag estimate on the training</span>
<span class="sd">        set. If n_estimators is small it might be possible that a data point</span>
<span class="sd">        was never left out during the bootstrap. In this case,</span>
<span class="sd">        `oob_decision_function_` might contain NaN.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized </span>
<span class="sd">           trees&quot;, Machine Learning, 63(1), 3-42, 2006.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.</span>
<span class="sd">    RandomForestClassifier : Ensemble Classifier based on trees with optimal</span>
<span class="sd">        splits.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;gini&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExtraTreesClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">ExtraTreeClassifier</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>


<span class="k">class</span> <span class="nc">ExtraTreesRegressor</span><span class="p">(</span><span class="n">ForestRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An extra-trees regressor.</span>

<span class="sd">    This class implements a meta estimator that fits a number of</span>
<span class="sd">    randomized decision trees (a.k.a. extra-trees) on various sub-samples</span>
<span class="sd">    of the dataset and uses averaging to improve the predictive accuracy</span>
<span class="sd">    and control over-fitting.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;forest&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : integer, optional (default=10)</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.20</span>
<span class="sd">           The default value of ``n_estimators`` will change from 10 in</span>
<span class="sd">           version 0.20 to 100 in version 0.22.</span>

<span class="sd">    criterion : string, optional (default=&quot;mse&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;mse&quot; for the mean squared error, which is equal to variance</span>
<span class="sd">        reduction as feature selection criterion, and &quot;mae&quot; for the mean</span>
<span class="sd">        absolute error.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Mean Absolute Error (MAE) criterion.</span>

<span class="sd">    max_depth : integer or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=&quot;auto&quot;)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float,</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    bootstrap : boolean, optional (default=False)</span>
<span class="sd">        Whether bootstrap samples are used when building trees.</span>

<span class="sd">    oob_score : bool, optional (default=False)</span>
<span class="sd">        Whether to use out-of-bag samples to estimate the R^2 on unseen data.</span>

<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeRegressor</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>

<span class="sd">    oob_prediction_ : array of shape = [n_samples]</span>
<span class="sd">        Prediction computed with out-of-bag estimate on the training set.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized trees&quot;,</span>
<span class="sd">           Machine Learning, 63(1), 3-42, 2006.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.</span>
<span class="sd">    RandomForestRegressor: Ensemble regressor using trees with optimal splits.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExtraTreesRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">ExtraTreeRegressor</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>


<span class="k">class</span> <span class="nc">RandomTreesEmbedding</span><span class="p">(</span><span class="n">BaseForest</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An ensemble of totally random trees.</span>

<span class="sd">    An unsupervised transformation of a dataset to a high-dimensional</span>
<span class="sd">    sparse representation. A datapoint is coded according to which leaf of</span>
<span class="sd">    each tree it is sorted into. Using a one-hot encoding of the leaves,</span>
<span class="sd">    this leads to a binary coding with as many ones as there are trees in</span>
<span class="sd">    the forest.</span>

<span class="sd">    The dimensionality of the resulting representation is</span>
<span class="sd">    ``n_out &lt;= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,</span>
<span class="sd">    the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;random_trees_embedding&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : integer, optional (default=10)</span>
<span class="sd">        Number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.20</span>
<span class="sd">           The default value of ``n_estimators`` will change from 10 in</span>
<span class="sd">           version 0.20 to 100 in version 0.22.</span>

<span class="sd">    max_depth : integer, optional (default=5)</span>
<span class="sd">        The maximum depth of each tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` is the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` is the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float,</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    sparse_output : bool, optional (default=True)</span>
<span class="sd">        Whether or not to return a sparse CSR matrix, as default behavior,</span>
<span class="sd">        or to return a dense array compatible with dense pipeline operators.</span>

<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeClassifier</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized trees&quot;,</span>
<span class="sd">           Machine Learning, 63(1), 3-42, 2006.</span>
<span class="sd">    .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  &quot;Fast discriminative</span>
<span class="sd">           visual codebooks using randomized clustering forests&quot;</span>
<span class="sd">           NIPS 2007</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;mse&#39;</span>
    <span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">sparse_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RandomTreesEmbedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">ExtraTreeRegressor</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_output</span> <span class="o">=</span> <span class="n">sparse_output</span>

    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;OOB score not supported by tree embedding&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit estimator.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape=(n_samples, n_features)</span>
<span class="sd">            The input samples. Use ``dtype=np.float32`` for maximum</span>
<span class="sd">            efficiency. Sparse matrices are also supported, use sparse</span>
<span class="sd">            ``csc_matrix`` for maximum efficiency.</span>

<span class="sd">        sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit estimator and transform dataset.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape=(n_samples, n_features)</span>
<span class="sd">            Input data used to build forests. Use ``dtype=np.float32`` for</span>
<span class="sd">            maximum efficiency.</span>

<span class="sd">        sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_transformed : sparse matrix, shape=(n_samples, n_out)</span>
<span class="sd">            Transformed dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csc&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Pre-sort indices to avoid that each individual tree of the</span>
            <span class="c1"># ensemble sorts the indices.</span>
            <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

        <span class="n">rnd</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RandomTreesEmbedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                              <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">one_hot_encoder_</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sparse_output</span><span class="p">,</span>
                                              <span class="n">categories</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_hot_encoder_</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transform dataset.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape=(n_samples, n_features)</span>
<span class="sd">            Input data to be transformed. Use ``dtype=np.float32`` for maximum</span>
<span class="sd">            efficiency. Sparse matrices are also supported, use sparse</span>
<span class="sd">            ``csr_matrix`` for maximum efficiency.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_transformed : sparse matrix, shape=(n_samples, n_out)</span>
<span class="sd">            Transformed dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_hot_encoder_</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016 - 2017, G. Lemaitre, F. Nogueira, D. Oliveira, C. Aridas

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/js/copybutton.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>