

import ast

import numpy as np
from collections import OrderedDict

from sklearn import model_selection
import sklearn.decomposition as decomposition
import sklearn.discriminant_analysis as discriminant_analysis
import sklearn.ensemble as ensemble
import sklearn.gaussian_process as gaussian_process
import sklearn.linear_model as linear_model
import sklearn.kernel_ridge as kernel_ridge
import sklearn.naive_bayes as naive_bayes
import sklearn.neighbors as neighbors
import sklearn.svm as svm
import sklearn.tree as tree

import xgboost
import lightgbm
import catboost

def modelParamBuilder(self, results, modelIx):
    """
    Info:
        Description:
            Return dictionary of parameter : value pairs
            to be passed as kwarg to model. Used to efficiently
            reinstantiated specific model variants that
        Parameters:
            results : Pandas DataFrame
                DataFrame containing score summary generated by
                PowerGridSearcher
            modelIx : int
                Row index specific model described on results to 
                be turned into dictionary containing parameter : value
                pairs.
    """
    estimator = results.loc[modelIx][0]
    params = results.loc[modelIx][5:].dropna(axis = 0)
    
    # convert floats that are effectively ints to ints
    for ix in params.index:
        if not isinstance(params[ix], str):
            if int(params[ix]) == params[ix] and isinstance(params[ix], float):
                params[ix] = params[ix].astype(np.int64)

    return estimator, params.to_dict()

class SklearnHelper():
    def __init__(self, clf, seed = 0, params = None, nJobs = 2):
        # params['random_state'] = seed
        
        if clf not in [ensemble.GradientBoostingClassifier, ensemble.GradientBoostingRegressor, ensemble.AdaBoostClassifier, ensemble.AdaBoostRegressor\
                        , naive_bayes.BernoulliNB, naive_bayes.GaussianNB, svm.SVC]:
            params['n_jobs'] = nJobs
        self.clf = clf(**params)

    def train(self, x_train, y_train):
        self.clf.fit(x_train, y_train)

    def predict(self, x):
        return self.clf.predict(x)
    
    def fit(self, x, y):
        return self.clf.fit(x, y)
    
    # def feature_importances(self, x, y):
    #     print(self.clf.fit(x, y).feature_importances_)

    def feature_importances(self, x, y):
        return self.clf.fit(x, y).feature_importances_

def oofGenerator(self, clf, x_train, y_train, x_valid, nFolds = 10):
    # row counts
    ntrain = x_train.shape[0]
    nvalid = x_valid.shape[0]
    
    # kfold train/test index generator
    kf = model_selection.KFold(n_splits = nFolds)
        
    # create shell arrays for holding results
    oof_train = np.zeros((ntrain,))
    oof_valid = np.zeros((nvalid,))
    oof_valid_skf = np.empty((nFolds, nvalid))

    # iterate through all kfolds to train model, capture scores
    for i, (train_index, test_index) in enumerate(kf.split(x_train)):
        # set train/test observations based on KFold indices
        x_tr = x_train[train_index]
        y_tr = y_train[train_index]
        x_te = x_train[test_index]

        # train model based on training variables and labels
        clf.train(x_tr, y_tr)

        # Update segment of oof_train where indices match the indices of the observations 
        # used as test observations. These are the "out of fold" observations that we are not
        # considered in the training phase of the model
        oof_train[test_index] = clf.predict(x_te)
        
        # generate predictions using entire validation dataset, otherwise unused up to this point
        # and capture predictions for each folds
        oof_valid_skf[i, :] = clf.predict(x_valid)

    # determine average score of validations predictions
    oof_valid[:] = oof_valid_skf.mean(axis = 0)
    return oof_train.reshape(-1, 1), oof_valid.reshape(-1, 1)

def paramExtractor(self, resultsDf, estimator, iteration):
    """

    """
    params = resultsDf[(resultsDf['estimator'] == estimator) & (resultsDf['iteration'] == iteration)]['params'].values[0]
    return ast.literal_eval(params)

def modelStacker(self, models, resultsDf, XTrain, yTrain, XValid, nFolds, nJobs):
    """

    """
    
    columns = []
    # iterate through estimators
    for estimator in models.keys():
        # iterate through parameter set for estimator
        for iteration in models[estimator]:
            print(estimator + ' ' + str(iteration))
            params = self.paramExtractor(resultsDf = resultsDf, estimator = estimator, iteration = iteration)
            columns.append(estimator + '_' + str(iteration))

            model = SklearnHelper(clf = eval(estimator), params = params, nJobs = nJobs)
            oofTrainModel, oofValidModel = self.oofGenerator(clf = model
                                                              ,x_train = XTrain
                                                              ,y_train = yTrain
                                                              ,x_valid = XValid
                                                              ,nFolds = nFolds
                                                             )
            try:
                oofTrain = np.hstack((oofTrain, oofTrainModel))
                oofValid = np.hstack((oofValid, oofValidModel))        
            except NameError:
                oofTrain = oofTrainModel
                oofValid = oofValidModel
    return oofTrain, oofValid, columns
