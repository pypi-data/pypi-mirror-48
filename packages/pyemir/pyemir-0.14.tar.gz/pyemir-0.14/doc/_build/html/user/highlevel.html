<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>High Level description of the EMIR Data Reduction Pipeline &#8212; PyEmir 0.9.dev1 documentation</title>
    
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.9.dev1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Engineering Recipes" href="engineering.html" />
    <link rel="prev" title="Image data products" href="images.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="engineering.html" title="Engineering Recipes"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="images.html" title="Image data products"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PyEmir 0.9.dev1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">PyEmir User Guide</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="high-level-description-of-the-emir-data-reduction-pipeline">
<h1>High Level description of the EMIR Data Reduction Pipeline<a class="headerlink" href="#high-level-description-of-the-emir-data-reduction-pipeline" title="Permalink to this headline">¶</a></h1>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">author:</th><td class="field-body">Nicolás Cardiel</td>
</tr>
<tr class="field-even field"><th class="field-name">revision:</th><td class="field-body">1</td>
</tr>
<tr class="field-odd field"><th class="field-name">date:</th><td class="field-body">Nov 23, 2017</td>
</tr>
</tbody>
</table>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>This is an overall description of the relevant processes involved in the Data Reduction Pipeline of
EMIR (basically from the point of view of the astronomers). In this sense, the information
here contained follows the contents of the <a class="reference external" href="http://guaix.fis.ucm.es/projects/emir/attachment/wiki/private/HighLevel/EMIR_ObservingStrategies.pdf">EMIR Observing Strategies</a> document.</p>
<div class="section" id="the-reason-for-a-data-reduction">
<h3>The reason for a data reduction<a class="headerlink" href="#the-reason-for-a-data-reduction" title="Permalink to this headline">¶</a></h3>
<p>The data reduction process, aimed to minimize the impact of data acquisition
imperfections on the measurement of data properties with a scientific meaning for the
astronomer, is typically performed by means of arithmetical manipulations of data
and calibration frames.</p>
<p>The imperfections are usually produced by non-idealities of image the sensors: temporal noise,
fixed pattern noise, dark current, and spatial sampling, among others. Although appropriate observational
strategies can greatly help in reducing the sources of data biases, the unavoidable limited observation
time that can be spent in each target determines the maximum signal-to-noise ratio in practice achievable.</p>
</div>
<div class="section" id="sources-of-errors">
<h3>Sources of errors<a class="headerlink" href="#sources-of-errors" title="Permalink to this headline">¶</a></h3>
<dl class="docutils">
<dt>Common sources of noise in image sensors are separated into two categories:</dt>
<dd><ul class="first last simple">
<li><strong>Random noise</strong>: This type of noise is temporally random (it is not constant from frame to frame).
Examples of this kind of noise are the followings:<ul>
<li><strong>Shot noise</strong>: It is due to the discrete nature of electrons. Two main phenomena contribute
to this source of error: the random arrival of photo-electrons to the detector (shot noise of
the incoming radiation), and the thermal generation of electrons (shot noise
of the dark current). In both cases, the noise statistical distributions are
well described by a Poisson distribution.</li>
<li><strong>Readout noise</strong>: Also known as floor noise, it gives an indication of the minimum
resolvable signal (when dark current is not the limiting factor), and accounts for
the amplifier noise, the reset noise, and the analog-to-digital converter noise.</li>
</ul>
</li>
<li><strong>Pattern noise</strong>: it is usually similar from frame to frame (i.e. it is stable
in larger timescales), and cannot be reduced by frame averaging.
Pattern noise is also typically divided into two subtypes:<ul>
<li><strong>Fixed Pattern Noise</strong> (FPN):This is the component of pattern
noise measured in absence of illumination. It is possible to compensate
for FPN by storing the signal generated under zero illumination and
subtracting it from subsequent signals when required.</li>
<li><strong>Photo-Response Non-Uniformity</strong> (PRNU): It is the component of pattern
noise that depends on the illumination (e.g. gain non-uniformity).
A first approximation is to assume that its contribution is a (small)
fraction, <img class="math" src="../_images/math/15fc867b8db831dea06536ee1c81db2aaecabdd8.png" alt="f_{\mathrm{PRNU}}"/>, of the number of photo-electrons, <img class="math" src="../_images/math/6794cad400be41a95d56bfc5fd2781421aca0c99.png" alt="N_e"/>.
Under this hypothesis, and considering in addition only the shot
noise due to photo-electrons, the resulting variance of the combination
of both sources of noise would be expressed as
<img class="math" src="../_images/math/e34e2ec55cfe6a5d7202dcaba941cf72dcbd4b9d.png" alt="N_e + (f_{\mathrm{PRNU}} * N_e)^2"/>
Thus, the worst case is obtained when <img class="math" src="../_images/math/6794cad400be41a95d56bfc5fd2781421aca0c99.png" alt="N_e"/> approaches
the pixel full-well capacity.</li>
</ul>
</li>
</ul>
</dd>
</dl>
<p>It is important to note that the correction of data biases, like the FPN,
also constitutes, by itself, a source of random error, since they are
performed with the help of a limited number of calibration images. In
the ideal case, the number of such images should be large enough to guarantee
that these new error contributors are negligible in comparison with the
original sources of random error.</p>
</div>
<div class="section" id="the-treatment-of-errors-in-the-data-reduction-process">
<h3>The treatment of errors in the data reduction process<a class="headerlink" href="#the-treatment-of-errors-in-the-data-reduction-process" title="Permalink to this headline">¶</a></h3>
<div class="section" id="three-methods-to-quantify-random-errors">
<h4>Three methods to quantify random errors<a class="headerlink" href="#three-methods-to-quantify-random-errors" title="Permalink to this headline">¶</a></h4>
<p>In a classic view a typical data reduction pipeline can be considered as
a collection of filters, each of which transforms input images into new output
images, after performing some kind of arithmetic manipulation and making
use of additional measurements and calibration frames when required.
Under this picture, three different approaches can in principle be
employed to determine random errors in completely reduced images.</p>
<div class="section" id="comparison-of-independent-repeated-measurements">
<h5>Comparison of independent repeated measurements<a class="headerlink" href="#comparison-of-independent-repeated-measurements" title="Permalink to this headline">¶</a></h5>
<p>This is one of the simplest and most straightforward ways to
estimate errors, since, in practice, errors are not computed
nor handled during the reduction procedure, but through the
comparison of the end products of the data processing. The only
requirement is the availability of a non too small number of
independent measurements. Although as such can be considered even
the flux collected by each <strong>independent</strong> pixel in a
detector (for example when determining the sky flux error in direct
imaging), in most cases this method requires the comparison of
different frames. For that reason, and given that for
many purposes it may constitute an extremely expensive method in
terms of observing time, its applicability on a general situation
seems rather unlikely.</p>
<a class="reference internal image-reference" href="../_images/method1.jpg"><img alt="method 1" src="../_images/method1.jpg" style="width: 800px;" /></a>
</div>
<div class="section" id="first-principles-and-brute-force-error-bootstrapping">
<h5>First principles and brute force: error bootstrapping<a class="headerlink" href="#first-principles-and-brute-force-error-bootstrapping" title="Permalink to this headline">¶</a></h5>
<p>Making use of the knowledge concerning how photo-electrons are generated (
expected statistical distribution of photon arrival into each pixel, detector
gain and read-out noise), it is possible to generate an error image associated
to each raw-data frame. In this sense, typically one can compute such error
image (in number of counts, ADU, &#8212;analogic to digital number&#8212;) as:</p>
<div class="math">
<p><img src="../_images/math/bf437bd45e3d48c70c3fca442fafa4abd8bb6f03.png" alt="\sigma_A(i,j)^2 = \frac{1}{g} A(i,j) + [f_{\mathrm{PRNU}} A(i,j)]^2 + \mathrm{RN}^2(i,j)"/></p>
</div><p>where <img class="math" src="../_images/math/96eedb1e37ed50d55e342b6e2513d908c67111f8.png" alt="A(i,j)"/> is the signal (after the bias-level subtraction)
in the pixel (i,j) of a given two-dimensional image (in ADU), <img class="math" src="../_images/math/67f4714f065d485540ad40829e0717bf75e9dd85.png" alt="g"/> is the gain
of the A/D converter (in e<sup>-</sup>/ADU), <img class="math" src="../_images/math/15fc867b8db831dea06536ee1c81db2aaecabdd8.png" alt="f_{\mathrm{PRNU}}"/> is the photo-response non-uniformity
factor discussed above, and <img class="math" src="../_images/math/8de4ff0f813c47c19f40842495c92cee0abd1675.png" alt="RN"/> is the read-out noise (in ADU).
Note that the apparent dimensional inconsistency of the previous expression is not real,
and arises from the fact that one of the properties of the Poisson distribution is that
its variance is numerically equal to the mean expected number of events.</p>
<p>By means of
error bootstrapping via Monte Carlo simulations, simulated initial data frames can be
generated and be completely reduced as if they were real observations. In order to
achieve this task, it is possible to use:</p>
<div class="math">
<p><img src="../_images/math/f4661a5f9de8c2033f5b17d5636851b7b06f4c7f.png" alt="A_{simul}(i,j)=A(i,j) + \sqrt{2}\sigma_A(i,j) \sqrt{-\log(1-z_1) \cos(2 \pi z_2)}"/></p>
</div><p>where <img class="math" src="../_images/math/295634395ccfc0b81efb969dee0caa08c9a10caa.png" alt="A_{simul}(i,j)"/> is a new instance of the initial raw-data frame,
and <img class="math" src="../_images/math/92c25fa25f006cdfe8ef28f3d3141e558aaac740.png" alt="z_1"/> and <img class="math" src="../_images/math/223e963ad95ab6098315774006a2d46397fb5f93.png" alt="z_2"/> are two random numbers in the range  [0,1).
Note that the second term in the right hand side of the previous expression introduces
Gaussian noise in each pixel. The comparison of the measurements performed over the whole
set of reduced simulated observations provides then a good estimation of the final errors.
However, and although this method overcome the problem of wasting observing time,
it can also be terribly expensive, but now in terms of computing time.</p>
<a class="reference internal image-reference" href="../_images/method2.jpg"><img alt="method 2" src="../_images/method2.jpg" style="width: 800px;" /></a>
</div>
<div class="section" id="first-principles-and-elegance-parallel-reduction-of-data-and-error-frames">
<h5>First principles and elegance: parallel reduction of data and error frames<a class="headerlink" href="#first-principles-and-elegance-parallel-reduction-of-data-and-error-frames" title="Permalink to this headline">¶</a></h5>
<p>Instead of wasting either observing or computing time, it is also possible to
feed the data reduction pipeline with both, the original raw-data frame and its
associated error frame (computed from first principles), and proceed only once
throughout the whole reduction process. In this case every single arithmetic manipulation
performed over the data image must be translated, using the law of
propagation of errors, into parallel manipulations of the error image.</p>
<p>Unfortunately, typical astronomical data reduction packages (e.g. Iraf, Midas, etc.)
do not consider random error propagation as a <strong>by default</strong> operation and, thus,
some kind of additional programming is unavoidable.</p>
<a class="reference internal image-reference" href="../_images/method3.jpg"><img alt="method 3" src="../_images/method3.jpg" style="width: 800px;" /></a>
</div>
</div>
<div class="section" id="error-correlation-a-real-problem">
<h4>Error correlation: a real problem<a class="headerlink" href="#error-correlation-a-real-problem" title="Permalink to this headline">¶</a></h4>
<p>Although each of the three methods described above is suitable of being
employed in different circumstances, the third approach is undoubtedly the one that,
in practice, can be used in a more general situation. In fact, once the appropriate data
reduction tool is available, the parallel reduction of data and error frames
is the only way to proceed when observing or computing time demands
are prohibitively high. However, due to the unavoidable fact that the information
collected by detectors is physically sampled in pixels, this approach collides with a
major problem: errors start to be correlated as soon as one introduces image
manipulations involving rebinning or non-integer pixel shifts of data.</p>
<p>A naive use of the analysis tools would neglect the effect of covariance terms, leading
to dangerously underestimated final random errors. Actually, this is likely the
most common situation since, initially, the classic reduction operates as
a black box, unless specially modified for the contrary. The figure below
shows a very simple example which illustrates this problem. Unfortunately, as
soon as one accumulates a few reduction steps involving increment of correlation
between adjacent pixels (e.g. image rectification when correcting for geometric
distortions, wavelength calibration into a linear scale, etc.), the number of
covariance terms starts to increase too rapidly to make it feasible the
possibility of stacking up and propagate all the new coefficients for every
single pixel of an image.</p>
<a class="reference internal image-reference" href="../_images/correlation.jpg"><img alt="Correlation" src="../_images/correlation.jpg" style="width: 800px;" /></a>
<p>In this simple example we illustrate the problem of error correlation when reducing data.
Assuming we have a linear detector, composed by a set of consecutive pixels, in an ideal
situation we are considering that all the signal of a given object (100 +/- 10 counts) is
received in a single pixel (we are ignoring additional sources of error, like read-out noise).
However, a small shift in the focal plane may imply that the observed signal
is distributed in two adjacent pixels. After reducing the data while restoring the image,
and propagating the observed errors in each pixel, the error in the total flux F is
computed using the errors in each pixel and following the law of combination of errors.
But if we use the incomplete expression, neglecting the covariance terms, we get an
unrealistic (and underestimated) error.</p>
</div>
<div class="section" id="a-modified-reduction-procedure">
<h4>A modified reduction procedure<a class="headerlink" href="#a-modified-reduction-procedure" title="Permalink to this headline">¶</a></h4>
<p>Obviously, the problem can be circumvented if one prevents its emergence, i.e. if one does
not allow the data reduction process to introduce correlation into neighbouring pixels
before the final analysis. In other words, if all the reduction steps that lead to error
correlation are performed in a single step during the measurement of the image properties
with a scientific meaning for the astronomer, there are no previous covariance
terms to be concerned with. Whether this is actually possible or not may depend
on the type of reduction steps under consideration. In any case, a change in
the philosophy of the classic reduction procedure can greatly help in alleviating
the problem. The core of this change consists in considering the reductions steps
that originate pixel correlation as filters that <strong>do not necessarily</strong> take
input images and generate new versions of them after applying some kind of
arithmetic manipulation, but as filters that properly <strong>characterize</strong>
the image properties, without modifying those input images.</p>
<p>More precisely, the reduction steps can be segregated in two groups:</p>
<blockquote>
<div><ul class="simple">
<li><strong>Simple filters</strong>, which do not require data rebinning nor non-integer pixel shifts of data.</li>
<li><strong>Complex filters</strong>, those suitable of introducing error correlation between adjacent pixels.</li>
</ul>
</div></blockquote>
<a class="reference internal image-reference" href="../_images/newreduction.jpg"><img alt="New reduction" src="../_images/newreduction.jpg" style="width: 800px;" /></a>
<p>The former may be operated like in a classic reductions, since their
application do not introduce covariance terms. However, the complex steps are only
allowed to determine the required image properties that one would need to actually
perform the correction. For the more common situations, these characterizations may
be simple polynomials (in order to model geometric distortions, non-linear wavelength
calibration scales, differential refraction dependence with wavelength, etc.).
Under this view, the end product of the modified reduction procedure is constituted
by a slightly modified version of the raw data frames after quite simple arithmetic
manipulations (denoted as <strong>raw data</strong> and <strong>raw errors</strong> in the previous figure), and
by an associated collection of image characterizations.</p>
</div>
<div class="section" id="modus-operandi">
<h4>Modus Operandi<a class="headerlink" href="#modus-operandi" title="Permalink to this headline">¶</a></h4>
<p>Clearly, at any moment it is possible to combine the result of the partial reduction
after all the linkable simple steps, with the information achieved through all the
characterizations derived from the complex steps, to obtain the same result than
in a classic data reduction (thick line in the previous figure).</p>
<p>However this is not the only option. Instead of trying to obtain completely reduced images
ready for starting the analysis work, one can directly feed a <strong>clever analysis tool</strong>
with the end products of the modified reduction procedure, as depicted in this figure:</p>
<div class="figure">
<a class="reference internal image-reference" href="../_images/cleverreduction.jpg"><img alt="Clever reduction" src="../_images/cleverreduction.jpg" style="width: 800px;" /></a>
</div>
<p>Obviously, this clever analysis tool has to perform its task taking into
account that some reductions steps have not been performed. For instance,
if one considers the study of a 2D spectroscopic image, the analysis tool should
use the information concerning geometric distortions, wavelength calibration
scale, differential refraction, etc., to obtain, for example, an equivalent
width through the measurement in the partially reduced (uncorrected for
geometric distortions, wavelength calibration, etc.) image.</p>
</div>
<div class="section" id="image-distortions-and-errors">
<h4>Image distortions and errors<a class="headerlink" href="#image-distortions-and-errors" title="Permalink to this headline">¶</a></h4>
<p>Interestingly, the most complex reduction steps are generally devoted to
compensate for image imperfections that can be associated with geometric distortions.
For illustration, and using the typical problems associated to the reduction of
long-slit spectroscopy, we can summarize the most common image distortions
in the following types:</p>
<blockquote>
<div><ul class="simple">
<li><em>Optical distortion</em>: Along the slit (spatial) direction, this distortion
would be equivalent to a geometric distortion in imaging mode.
Furthermore, this distortion also includes any possible spatial distortion
of the spectra in the detector (i.e. spectra of punctual objects not following a
line parallel to the detector rows) which is not due to the slit in use
(orientation or shape defects; see below) or to refraction effects.
The way to deduce the distortion map (note that it is a 3D map,
accounting the third dimension for the distortion of the spectra)
is by observing punctual objects in different positions of the focal plane.
This can be accomplished by observing lamp arc spectra through special
masks with evenly distributed holes along a focal plane column.</li>
<li><em>Slit distortion</em>: This distortion accounts for the potential distortions
introduced by the use of an imperfect slit. This includes:<ol class="loweralpha">
<li>small variations in the slit width along the slit direction and,</li>
<li>the difference in slit orientation with respect to the vertical direction
in the detector plane.</li>
</ol>
</li>
<li><em>Wavelength distortion</em>: Commonly referred as wavelength calibration,
this distortion accounts for the fact that the relation between pixels
and actual wavelengths along the dispersion direction, after the removal
of the two previous distortions, is typically not linear.</li>
<li><em>Differential refraction distortion</em>: In the absence of the three previous
distortions, the dependence of atmospheric dispersion with wavelength
produces that the spectrum of a punctual source does not follow a
straight line parallel to the dispersion direction. This effect depends
mainly on the zenith angle of the observation, the wavelength range,
and the difference between the slit position angle and the parallactic
angle (being the distortion maximum when both angles are the same, and zero
if they are orthogonal). For these reasons, it is not possible to
derive a general distortion map for a given instrument setup, but this
kind of distortion must be corrected individually for each observed frame.</li>
</ul>
</div></blockquote>
<p>To accomplish a proper random error treatment, as previously described, it is
necessary to manipulate the data using a new and distorted system of coordinates
that must account for all the image distortions present in the data.
These distortions should be easily mapped with the help of calibration images.
The new coordinate system provides the correspondence between the expected scientific
coordinate system (e.g. wavelength and 1D physical size, in spectroscopic
observations) and the observed coordinate system (physical pixels).</p>
<p>It is important to highlight that, in this situation, the error estimation
should not be a complex task, since the analysis tool is supposed
to be handling uncorrelated pixels.</p>
<p>The bottom line that can be extracted from the comparison of the different
methods to estimate random errors in data reduction processes is the
relevance of delaying the arithmetic manipulations involving the
rebinning of the data until their final analysis.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In the case of EMIR, we will use the parallel reduction of data and error frames,
trying to combine the arithmetical manipulations implying signal rebinning into the fewer
steps as possible. In this way we hope to minimize the impact of error correlation.
If we have enough time, we can try to create software tools that perform the kind of
<em>clever analysis</em> we have previously described.</p>
</div>
</div>
</div>
</div>
<div class="section" id="basic-observing-modes-and-strategies">
<h2>Basic observing modes and strategies<a class="headerlink" href="#basic-observing-modes-and-strategies" title="Permalink to this headline">¶</a></h2>
<p>EMIR is offering two main observing modes:</p>
<blockquote>
<div><ul class="simple">
<li><strong>imaging</strong>: FOV of 6.67 x 6.67 arcmin, with a plate scale of 0.2 arcsec/pixel.
Imaging can be done through NIR broad-band filters Z, J, H, K, K<sub>s</sub>, and a
dataset of narrow-band filters (TBC).</li>
<li><strong>multi-object spectroscopy</strong>: multi-slit mask with a FOV of 6.67 x 4 arcmin.
Long-slit spectroscopy can be performed by placing the slitlets in adjacent positions.</li>
</ul>
</div></blockquote>
<p>We are assuming that a particular observation is performed by obtaining a set of images,
each of which is acquired at different positions referred as offsets from the base
pointing. In this sense, and following the notation used
in <a class="reference external" href="http://guaix.fis.ucm.es/projects/emir/attachment/wiki/private/HighLevel/EMIR_ObservingStrategies.pdf">EMIR Observing Strategies</a>, several situations are considered:</p>
<blockquote>
<div><ul class="simple">
<li><strong>Telescope</strong></li>
<li><strong>Chopping</strong> (TBD if this option will be available): achieved by
moving the GTC secondary mirror. It provides a 1D move of the order
of 1 arcmin. The purpose is to isolate the source flux from the sky
background flux by first measuring the total (Source+Background) flux
and then subtracting the signal from the Background only.</li>
<li><strong>DTU Offseting</strong>: the Detector Translation Unit allows 3D movements
of less than 5 arcsec. The purpose is the same as in the chopping case,
<strong>when the target is point-like</strong>. It might also be used to defocus
the target for photometry or other astronomical uses.</li>
<li><strong>Dither</strong>: it is carried out by pointing to a number of pre-determined
sky positions, with separations of the order of 25 arcsec, using
the GTC primary or secondary mirrors, or the EMIR DTU, or the
Telescope. The purpose of this observational strategy is to
avoid saturating the detector, to allow the removal of cosmetic
defects, and to help in the creation of a sky frame.</li>
<li><strong>Nodding</strong>: pointing the Telescope alternatively between
two or more adjacent positions on a 1D line, employing low frequency
shifts and typical distances of the order of slitlet-lengths
(it plays the same role as chopping in imaging).</li>
<li><strong>Jitter</strong>: in this case the source falls randomly around
a position in a known distribution, with shifts typically
below 10 arcsec, to avoid cosmetic defects.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="imaging-mode">
<h2>Imaging Mode<a class="headerlink" href="#imaging-mode" title="Permalink to this headline">¶</a></h2>
<div class="topic">
<p class="topic-title first">Inputs</p>
<ul class="simple">
<li>Science frames</li>
<li>Offsets between them</li>
<li>Master Dark</li>
<li>Bad pixel mask (BPM)</li>
<li>Non-linearity correction polynomials</li>
<li>Master flat</li>
<li>Master background</li>
<li>Exposure Time (must be the same in all the frames)</li>
<li>Airmass for each frame</li>
<li>Detector model (gain, RN)</li>
<li>Average extinction in the filter</li>
</ul>
</div>
<p>In near-infrared imaging it is important to take into account
that the variations observed in the sky flux in a given image are
due to real spatial variations of the sky brightness along the
field of view, the thermal background, and intrinsic flatfield variations.</p>
<p>The master flatfield can be computed from the same science
frames (for small targets) or from adjacent sky frames.
This option, however, is not the best one, since the sky brightness
is basically produced by a finite subset of bright emission lines,
which SED is quite different from a continuous source. For this
reason, most of the times the preferred master flatfield should
be computed from twilight flats. On the other hand, systematic
effects are probably more likely in this second approach.
Probably it will be required to test both alternatives.
The description that follows describes the method employed
when computing the master flatfield from the same set of night images,
at is based on the details given in <a class="reference external" href="http://guaix.fis.ucm.es/projects/emir/attachment/wiki/private/HighLevel/scam_20001113.pdf">SCAM reduction document</a>,
corresponding to the reduction of images obtained with NIRSPEC at Keck II.</p>
<p>A typical reduction scheme for imaging can be the following:</p>
<blockquote>
<div><ul class="simple">
<li>Data modelling (if appropriate/possible) and variance frame creation from first principles: all the frames</li>
<li>Correction for non-linearity: all the frames<ul>
<li>Data: <img class="math" src="../_images/math/8025581794b1fdacd82f067acc6f6a3c7faa61b0.png" alt="I_{\mathrm{linear}}(x,y)=I_{\mathrm{observed}}(x,y) \times \mathrm{Pol}_{\mathrm{linearity}}"/></li>
<li>Variances: <img class="math" src="../_images/math/19bf18d34edfadd7a125fde136d2ed4432c7b819.png" alt="\sigma^2_{\mathrm{linear}}(x,y)=[\sigma_{\mathrm{model}}(x,y) \mathrm{Pol}_{\mathrm{linearity}}]^2 + [I_{\mathrm{observed}}(x,y) \mathrm{ErrorPol}_{\mathrm{linearity}}]^2"/></li>
</ul>
</li>
<li>Dark correction: all the frames<ul>
<li>Data: <img class="math" src="../_images/math/1061a8e436a5c9a622321be804ca9bbcc875dadd.png" alt="I_{\mathrm{dark}}(x,y)=I_{\mathrm{linear}}(x,y)- \mathrm{MasterDark}(x,y)"/></li>
<li>Variances: <img class="math" src="../_images/math/b925c40c537763828ce732311e9048feee106087.png" alt="\sigma^2_{dark}(x,y)=[\sigma_{linear}(x,y)]^2 + [ErrorMasterDark(x,y)]^2"/></li>
</ul>
</li>
<li>Master flat and object mask creation: <em>a loop starts</em></li>
</ul>
</div></blockquote>
<p><strong>First iteration</strong>: computing the object mask, refining the telescope offsets, QC to the frames.</p>
<ul class="simple">
<li>No object mask is used (it is going to be computed).</li>
<li>All the dark-corrected science frames are used.</li>
<li>No variances computation.</li>
<li>BPM is used.</li>
</ul>
<blockquote>
<div><ol class="loweralpha simple">
<li>Flat computation (1st order): <img class="math" src="../_images/math/f2a3d2d9d10b5eeadc83f7aff1f6881a39e101bb.png" alt="Flat^{1st}(x,y)=\mathrm{Comb}[I_{dark}(x,y)]/\mathrm{Norm}"/></li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Combination using the median (alternatively, using the mean).</li>
<li>No offsets taken into account.</li>
<li>Normalization to the mean.</li>
</ul>
</div></blockquote>
<ol class="loweralpha simple" start="2">
<li>Flat correction (1st order): <img class="math" src="../_images/math/fa14bd497cceedaf4c6f0e32c83ac6dbea99ed5d.png" alt="I_{flat}^{1st}(x,y)= I_{dark}(x,y)/\mathrm{Flat}^{1st}(x,y)"/></li>
<li>Sky correction (1st order): <img class="math" src="../_images/math/e43d37acccaff824d4d8d65d09a95ff895a45e96.png" alt="I_{sky}^{1st}(x,y) = I_{flat}^{1st}(x,y)-Sky"/></li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Sky is computed and subtracted in each array channel (mode of all the
pixels in the channel), in order to avoid time-dependent variations of the channel amplifiers.</li>
<li>BPM is used for the above sky level determination.</li>
</ul>
</div></blockquote>
<ol class="loweralpha simple" start="4">
<li>Science image (1st order): <img class="math" src="../_images/math/6d3bc4ba8269e2be2ae012e975e74ac5ee00eba7.png" alt="Science^{1st}(x,y)=Comb[I_{sky}^{1st}(x,y)]"/></li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Combination using the median.</li>
<li>Taking telescope offsets into account.</li>
<li>Extinction correction is performed to each frame before combination:
<img class="math" src="../_images/math/854008e4d9de0f050fda4717c09464e8249bd145.png" alt="\times 10^{0.4 k X}"/>, being <img class="math" src="../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/> the airmass.</li>
<li>Rejection of bad pixels during the combination (alternatively, asigma-clipping algorithm).</li>
</ul>
</div></blockquote>
<ol class="loweralpha simple" start="5">
<li>Object Mask (1st order): <img class="math" src="../_images/math/f8f1f52adbfd1111dd3d0e10adf51b2c6b38539b.png" alt="SExtractor[Science^{1st}(x,y)] -&gt; Obj_Mask^1st(x,y)"/></li>
</ol>
<blockquote>
<div><ul class="simple">
<li>High DETECT_THRESH (for detecting only the brightest objects).</li>
<li>Saturation limit must be carefully set (detected objects must not be saturared).</li>
</ul>
</div></blockquote>
<ol class="loweralpha simple" start="6">
<li>Offsets refinement:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Objects are also found in the sky-corrected frames:
<img class="math" src="../_images/math/29042ba934accf5362577d7a7dd987c59e370e76.png" alt="SExtractor[I_{sky}^{1st}(x,y)]"/></li>
<li>All the objects detected in the combined science image are also identified
in each sky-corrected frame. For doing that, the position of each source
from the combined image is converted into positions in the reference
system of each frame <img class="math" src="../_images/math/dd589fb05648d007c868bed5d6be71678c217248.png" alt="I_{sky}^{1st}(x,y)"/>. The telescope offsets
are used for a first estimation of the source position in the frame.
A TBD cross-correlation algorithm finds the correct source position
into a window of size S around the estimated position.
The new improved offsets are computed for each source in each frame.</li>
<li>The differences between the improved offsets (OFFX, OFFY) and the telescope
(nominal) offsets (OFFX<sup>tel</sup>, OFFY<sup>tel</sup>) are computed
for each object in each frame.</li>
<li>The differences between both sets of offsets are plotted for all
the objects vs. Object Number, ordered by brightness.</li>
<li>The mean values of these differences (weighting with object brightness)
are computed, making an approximation to integer values.
These values represent the average displacement of the true offsets of the
frame relative to the nominal telescope offsets.</li>
<li>If the estimated refined offsets are very different from the nominal values,
the <img class="math" src="../_images/math/807511b7b448950705ea65491bc6804439268b74.png" alt="Science^{1st}(x,y)"/> image is computed again,
using the refined offset values. A llop starts from step d) to f),
until the offsets corrections are less than a TBD threshold
value for the corresponding frame.</li>
</ul>
</div></blockquote>
<ol class="loweralpha simple" start="7">
<li>Quality Control for the science frames:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>The brightest objects detected in the <img class="math" src="../_images/math/5b02187ac10f791e31f3248a4665b1e0329da15b.png" alt="ObjMask^{1st}(x,y)"/>
are selected (N~5 objects). They must appear in more than two frames.</li>
<li>The FLUX_AUTO and the FWHM of each selected object are computed in each frame.</li>
<li>The <img class="math" src="../_images/math/56c43bb9e1161a5df088bdfb4c6d3a6ba9755584.png" alt="\mathrm{FLUX\_AUTO} \times 10^{0.4 k X}"/> and FWHM are plotted vs. frame number.</li>
<li>The median values of <img class="math" src="../_images/math/56c43bb9e1161a5df088bdfb4c6d3a6ba9755584.png" alt="\mathrm{FLUX\_AUTO} \times 10^{0.4 k X}"/>
and FWHM along all the frames are computed for each object,
as well as their standard deviations.</li>
<li>A sigma-clipping algorithm will select those frames with more
than N/2 objects (TBD) lying +/- 1 sigma above/below the median value of <img class="math" src="../_images/math/56c43bb9e1161a5df088bdfb4c6d3a6ba9755584.png" alt="\mathrm{FLUX\_AUTO} \times 10^{0.4 k X}"/>.
These frames will be flagged as <strong>non-adequate</strong> for the
creation of the final science frame.</li>
<li>All those frames with FWHM lying n times sigma above their
median value or m times sigma below it are also flagged as <strong>non-adequate</strong>.
Notice that m and n must be different (FWHM values better than the median
must be allowed).</li>
<li>The <strong>non-adequate</strong> frames are not used for generating the final science
frame. They will be avoided in the rest of the reduction.</li>
<li>A QC flag will be assigned to the final science image, depending on the
number of frames finally used in the combination. E.g, QC_GOOD if
between 90-100% of the original set of frames are <strong>adequate</strong>,
QC_FAIR between 70-90%, QC_BAD below 70% (the precise numbers TBD).</li>
</ul>
</div></blockquote>
</div></blockquote>
<p><strong>Second iteration</strong></p>
<ul class="simple">
<li><img class="math" src="../_images/math/5b02187ac10f791e31f3248a4665b1e0329da15b.png" alt="ObjMask^{1st}(x,y)"/> is used for computing the flatfield and the sky.</li>
<li>Only those dark-corrected science frames that correspond to <strong>adequate</strong> frames are used.</li>
<li>No variances computation.</li>
<li>BPM is also used.</li>
</ul>
<blockquote>
<div><ol class="loweralpha simple">
<li>Flat computation (2nd order): <img class="math" src="../_images/math/fb97bf5b69e8fa56802cdff33764c146413463cc.png" alt="Flat^{2nd}(x,y)=Comb[I_{dark}(x,y)]/Norm"/></li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Combination using the median (alternatively, using the mean).</li>
<li>The first order object mask is used in the combination.</li>
<li>No offsets taken into account in the combination, although they are
used for translating positions in the object mask to positions in
each individual frame.</li>
<li>Normalization to the mean.</li>
</ul>
</div></blockquote>
<ol class="loweralpha simple" start="2">
<li>Flat correction (2nd order): <img class="math" src="../_images/math/aa56245ce5e438a649c404d32f4d6371342d7fdc.png" alt="I_{flat}^{2nd}(x,y)= I_{dark}(x,y)/Flat^{2nd}(x,y)"/></li>
<li>Sky correction (2nd order): <img class="math" src="../_images/math/dec15f48b84bcad92081246ede70682c113a84e4.png" alt="I_{sky}^{2nd}(x,y) = I_{flat}^{2nd}(x,y)-Sky^{new}(x,y)"/></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><img class="math" src="../_images/math/fd7f37719c6beb7bf05df0e2fc2ec63cb2592ca0.png" alt="Sky^{new}"/> is computed as the average of m (~ 6, TBD) <img class="math" src="../_images/math/7c315079019945e1a2d941157f26edda226181b9.png" alt="I_{flat}^{2nd}(x,y)"/>
frames, near in time to the considered frame, taking into account the first order
object mask and the BPM.</li>
<li>An array storing the number of values used for computing the sky in each pixel
is generated (weights array).</li>
<li>If no values are adequate for computing the sky in a certain pixel,
a zero is stored at the corresponding position in the weights array.
The sky value at these pixels is obtained through interpolation
with the neighbouring pixels.</li>
</ul>
</div></blockquote>
<ol class="loweralpha simple" start="4">
<li>Science image (2nd order): <img class="math" src="../_images/math/da4aac50df090666683624a8f1d191079dcf1c10.png" alt="Science^{2nd}(x,y)=Comb[I_{sky}^{2nd}(x,y)]"/></li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Combination using the median.</li>
<li>Taking the refined telescope offsets into account.</li>
<li>Extinction correction is performed to each frame before combination: <img class="math" src="../_images/math/854008e4d9de0f050fda4717c09464e8249bd145.png" alt="\times 10^{0.4 k X}"/>,
being <img class="math" src="../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/> the airmass.</li>
<li>Rejection of bad pixels during the combination (alternatively, asigma-clipping algorithm).</li>
</ul>
</div></blockquote>
<ol class="loweralpha simple" start="5">
<li>Object Mask (2nd order): <img class="math" src="../_images/math/c5e64ff50c477b937eedf4c92193c2fbfb939aef.png" alt="SExtractor[Science^{2nd}(x,y)] -&gt; ObjMask^{2nd}(x,y)"/></li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Lower DETECT_THRESH.</li>
<li>Saturation limit must be carefully set.</li>
</ul>
</div></blockquote>
</div></blockquote>
<p><strong>Third iteration</strong></p>
<ul class="simple">
<li><img class="math" src="../_images/math/27ac45661458348546eecd30b7725a49a3aa20c5.png" alt="ObjMask^{2nd}(x,y)"/> is used in the combinations.</li>
<li>Only those dark-corrected science frames that correspond to <strong>adequate</strong> frames are used.</li>
<li>Variance frames are computed.</li>
<li>BPM is also used.</li>
</ul>
<p><strong>Additional iterations</strong>: stop the loop when a suitable criterium applies (TBD).</p>
</div>
<div class="section" id="multi-object-spectroscopy-mode">
<h2>Multi-Object Spectroscopy Mode<a class="headerlink" href="#multi-object-spectroscopy-mode" title="Permalink to this headline">¶</a></h2>
<div class="topic">
<p class="topic-title first">Inputs</p>
<ul class="simple">
<li>Science frames</li>
<li>Offsets between them</li>
<li>Master Dark</li>
<li>Bad pixel mask (BPM)</li>
<li>Non-linearity correction polynomials</li>
<li>Master spectroscopic flat</li>
<li>Master spectroscopic background</li>
<li>Master wavelength calibration</li>
<li>Master spectrophotometric calibration</li>
<li>Exposure Time (must be the same in all the frames)</li>
<li>Airmass for each frame</li>
<li>Extinction correction as a function of wavelength</li>
<li>Detector model (gain, RN)</li>
</ul>
</div>
<p>In the case of EMIR, the reduction of the Multi-Object Spectroscopy observations
will be in practice carried out by extracting the individual aligned slits
(not necessarily single slits), and reducing them as if they were traditional
long-slit observations in the near infrared. Most of the steps to be applied
to these <strong>pseudo long-slit</strong> subimages are those graphically depicted in this figure</p>
<a class="reference internal image-reference" href="../_images/reduceme_spectra.jpg"><img alt="REDUCEME spectra" src="../_images/reduceme_spectra.jpg" style="width: 800px;" /></a>
<p>The details are given in <a class="reference external" href="http://www.ucm.es/info/Astrof/users/ncl/thesis/thesis3.ps.gz">Chapter 3</a> of Cardiel&#8217;s thesis (1999).
The key difference in the infrared observations is the sky subtraction, which
will depend on the observational strategy.</p>
<p>Basic steps must include:</p>
<blockquote>
<div><ul class="simple">
<li>Data modelling (if appropriate/possible) and variance frame creation
from first principles: all the frames</li>
<li>Correction for non-linearity: all the frames<ul>
<li>Data: <img class="math" src="../_images/math/7204a6870e5b60874e88cbb62273433fd092275d.png" alt="I_{linear}(x,y)=I_{observed}(x,y) Pol_{linearity}"/></li>
<li>Variances: <img class="math" src="../_images/math/a160287e1518a870630b1fb8f38482c5e6f41abb.png" alt="\sigma^2_{linear}(x,y)=[\sigma_{model}(x,y) Pol_{linearity}]^2 + [I_{observed}(x,y) ErrorPol_{linearity}]^2"/></li>
</ul>
</li>
<li>Dark correction: all the frames<ul>
<li>Data: <img class="math" src="../_images/math/394c64469e416af570b4e1a816b455a2606cab5c.png" alt="I_{dark}(x,y)=I_{linear}(x,y) - MasterDark(x,y)"/></li>
<li>Variances: <img class="math" src="../_images/math/b925c40c537763828ce732311e9048feee106087.png" alt="\sigma^2_{dark}(x,y)=[\sigma_{linear}(x,y)]^2 + [ErrorMasterDark(x,y)]^2"/></li>
</ul>
</li>
<li>Flatfielding: distinguish between high frequency (pixel-to-pixel) and
low-frequency (overall response and slit illumination) corrections.
Lamp flats are adequate for the former and twilight flats for the second. Follow section</li>
<li>Detection and extraction of slits: apply Border_Detection algorithm, from own
frames or from flatfields.</li>
<li>Cleaning<ul>
<li>Single spectroscopic image: sigma-clipping algorithm removing
local background in pre-defined direction(s).</li>
<li>Multiple spectroscopic images: sigma-clipping from comparison between frames.</li>
</ul>
</li>
<li>Wavelength calibration and C-distortion correction of each slit. Double-check with available sky lines.</li>
<li>Sky-subtraction (number of sources/slit will be allowed to be &gt; 1?).<ul>
<li>Subtraction using sky signal at the borders of the same slit.</li>
<li>Subtraction using sky signal from other(s) slit(s), not necessarily adjacent.</li>
</ul>
</li>
<li>Spectrophotometric calibration of each slit, using the extinction correction
curve and the master spectrophotometric calibration curve.</li>
<li>Spectra extraction: define optimal, average, peak, FWHM.</li>
</ul>
</div></blockquote>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/logo.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">High Level description of the EMIR Data Reduction Pipeline</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a><ul>
<li><a class="reference internal" href="#the-reason-for-a-data-reduction">The reason for a data reduction</a></li>
<li><a class="reference internal" href="#sources-of-errors">Sources of errors</a></li>
<li><a class="reference internal" href="#the-treatment-of-errors-in-the-data-reduction-process">The treatment of errors in the data reduction process</a><ul>
<li><a class="reference internal" href="#three-methods-to-quantify-random-errors">Three methods to quantify random errors</a><ul>
<li><a class="reference internal" href="#comparison-of-independent-repeated-measurements">Comparison of independent repeated measurements</a></li>
<li><a class="reference internal" href="#first-principles-and-brute-force-error-bootstrapping">First principles and brute force: error bootstrapping</a></li>
<li><a class="reference internal" href="#first-principles-and-elegance-parallel-reduction-of-data-and-error-frames">First principles and elegance: parallel reduction of data and error frames</a></li>
</ul>
</li>
<li><a class="reference internal" href="#error-correlation-a-real-problem">Error correlation: a real problem</a></li>
<li><a class="reference internal" href="#a-modified-reduction-procedure">A modified reduction procedure</a></li>
<li><a class="reference internal" href="#modus-operandi">Modus Operandi</a></li>
<li><a class="reference internal" href="#image-distortions-and-errors">Image distortions and errors</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#basic-observing-modes-and-strategies">Basic observing modes and strategies</a></li>
<li><a class="reference internal" href="#imaging-mode">Imaging Mode</a></li>
<li><a class="reference internal" href="#multi-object-spectroscopy-mode">Multi-Object Spectroscopy Mode</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="images.html"
                        title="previous chapter">Image data products</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="engineering.html"
                        title="next chapter">Engineering Recipes</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/user/highlevel.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="engineering.html" title="Engineering Recipes"
             >next</a> |</li>
        <li class="right" >
          <a href="images.html" title="Image data products"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PyEmir 0.9.dev1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" >PyEmir User Guide</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2010-2017, Sergio Pascual, Nicolás Cardiel.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.2.
    </div>
  </body>
</html>